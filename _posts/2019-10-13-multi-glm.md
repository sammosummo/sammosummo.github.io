---
layout: post
title: Easy Bayesian generalization of mulitvariate linear models
date: 2019-10-12
has_comments: true
has_code: true
has_math: true
include_references: true
references:
 - Breslow1993a
 - Salvatier2016a
 - Coull2000a
 - Carpenter2017a
 - Lewandowski2009a
---
Consider the multivariate version of the general linear model (GLM),

$$\begin{equation}
\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{B}+\boldsymbol{E}\\
\boldsymbol{E}\sim\textrm{MvNormal}\left(0, \boldsymbol{\Sigma}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{Y}$$ is a matrix of data with rows indicating independent cases and columns indicating different
variables; $$\boldsymbol{X}$$ is a matrix of covariates; $$\boldsymbol{B}$$ is a matrix of coefficients;
$$\boldsymbol{E}$$ is a matrix of residuals; and $$\boldsymbol{\Sigma}$$ is a covariance matrix.

To Bayesians, the following equivalent expression may feel more natural:

$$\begin{equation}
\boldsymbol{Y}\sim\textrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B},\boldsymbol{\Sigma}\right)\textrm{.}
\end{equation}$$

In the special case where $$\boldsymbol{\Sigma}$$ is a diagonal matrix, the above model is equivalent to multiple
univariate GLMs, one on each dependent variable. The univariate GLM can be written as

$$\begin{equation}
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\\
\boldsymbol{e}\sim\textrm{Normal}\left(0, \sigma^2\right)\textrm{,}
\end{equation}$$

or equivalently,

$$\begin{equation}
\boldsymbol{y}\sim\textrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, \sigma^2\right)\textrm{.}
\end{equation}$$

Univariate GLMs are simpler and more commonly used than multivariate GLMs. If a researcher doesn't care about potential
correlations between dependent variables, or how such correlations might influence estimates of regression coefficients,
they may as well use several univariate GLMs.

Suppose that the data in $$\boldsymbol{Y}$$ are categorical, ordinal, counts, or bounded between certain values. In such
cases, the residuals may not be well approximated by a normal distribution, making GLMs inappropriate.

When dealing with a single dependent variable, we can use a general*ized* linear model (GZLM). For example, for binary
data, we can use the following model:

$$\begin{equation}
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{.}
\end{equation}$$

In words, this model assumes that (1) the data are Bernoulli trials, and therefore that their likelihood is given by the
probability mass function of the Bernoulli distribution conditional on a set of success probabilities; and (2) the
vector of logits of these probabilities are equal to a linear combination of covariates and coefficients. One could
describe this model as the univariate logistic Bernoulli GZLM, but it is more commonly called *logistic regression*.

This is all fine, but what do we do if we have a multivariate data set, the correlations between dependent variables are
important, _and_ the residuals aren't well approximated by a normal distribution?

A common solution is to recast the data matrix $$\boldsymbol{Y}$$ as a single column and use a generalized linear mixed
model (GZLMM) where cases are random effects (e.g., [Breslow & Clayton, 1993](#Breslow1993a)). For example,

$$\begin{equation}
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{u}\right)\\
\boldsymbol{\vec{y}}=\mathrm{vec}\left(\boldsymbol{Y}\right)\\
\boldsymbol{\vec{y}}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{,}
\end{equation}$$

were $$\boldsymbol{Z}$$ is a design matrix and $$\boldsymbol{u}$$ is a vector of coefficients for the random effects.
These random effects act as separate offsets  (or random intercepts) for each case, which induces positive correlations
across items. The fixed-effect parts of the model, $$\boldsymbol{X}$$ and $$\boldsymbol{\beta}$$, need to be modified to
accommodate the shape of $$\boldsymbol{\vec{y}}$$ and to estimate the covariate effects separately for each item
(i.e., random slopes) if desired.

Another solution, considered here, is to define a model whose likelihood function is the probability
density/mass function of some conventional distribution conditional on a set parameters, and assume that some
transformation of these parameters follows a multivariate normal distribution whose expectation is a linear
combination of covariates and coefficients.

Importantly, this is not quite the same thing as a multivariate extension of a univariate GZLM, nor is it a GZLMM. To
illustrate, first consider the following univariate model:

$$\begin{equation}
\boldsymbol{\psi}\sim\mathrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, 1\right)\\
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{\psi}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{.}
\end{equation}$$

The difference between this model and logistic regression is the addition of a latent variable,
$$\boldsymbol{\psi}$$, whose expectation is $$\boldsymbol{X}\boldsymbol{\beta}$$. Hereafter, I refer to this class of
models as latent-variable models (LVMs). This particular example could be called a univariate logistic Bernoulli LVM. 

Within the frequentist framework, fitting univariate LVMs is not as easy as fitting univariate GZLMs because the former
are typically not included within common statistics packages. Fitting LVMs by hand is also potentially tricky because they
have unconventional likelihood functions. For example, whereas the likelihood function for logistic regression is
simply the probability mass function of the Bernoulli distribution after substituting $$\boldsymbol{\pi}$$ for
$$\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}\right)$$, the likelihood function of corresponding LVM
involves an additional step that marginalizes out $$\boldsymbol{\psi}$$. (I've omitted this equation from this post because, frankly, I don't
have the time or patience to derive and typeset it.)

Thankfully, within the Bayesian framework, we can side-step most of the additional difficulty associated with LVMs.
Bayesian software packages, such as PyMC3 [(Salvatier, Wiecki, & Fonnesbeck, 2016)](#Salvatier2016a) and Stan
[(Carpenter et al., 2017)](#Carpenter2017a), define models in terms of the probabilistic 
relationships between variables, so there is no need to write the unconventional likelihood functions explicitly. The
latent variables within LVMs are estimated using Markov Chain Monte Carlo (MCMC), along with the other variables in the
model.

Here is a demonstration. In the script below, I loaded a publicly available data set and fitted three models to it.
The first is a frequentist logistic regression. The second is a Bayesian logistic regression with weakly
informative priors. The third is a univariate logistic Bernoulli LVM. Notice that the code for the LVM is not all that
different from the Bayesian logistic regression. 

```python
{{ site.data.code.logistic_example__py }}
```

The portion of the output related to the first model is

```
Optimization terminated successfully.
         Current function value: 0.573147
         Iterations 6
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                  admit   No. Observations:                  400
Model:                          Logit   Df Residuals:                      394
Method:                           MLE   Df Model:                            5
Date:                Fri, 11 Oct 2019   Pseudo R-squ.:                 0.08292
Time:                        22:38:50   Log-Likelihood:                -229.26
converged:                       True   LL-Null:                       -249.99
Covariance Type:            nonrobust   LLR p-value:                 7.578e-08
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
gre            0.0023      0.001      2.070      0.038       0.000       0.004
gpa            0.8040      0.332      2.423      0.015       0.154       1.454
prestige_2    -0.6754      0.316     -2.134      0.033      -1.296      -0.055
prestige_3    -1.3402      0.345     -3.881      0.000      -2.017      -0.663
prestige_4    -1.5515      0.418     -3.713      0.000      -2.370      -0.733
intercept     -3.9900      1.140     -3.500      0.000      -6.224      -1.756
==============================================================================
```


The output related to the second model is

```
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 3 jobs)
NUTS: [betas]
              mean        sd  mc_error  ...  hpd_97.5         n_eff      Rhat
betas__0  0.002156  0.001078  0.000007  ...  0.004334  24447.135896  1.000059
betas__1  0.650231  0.307705  0.002104  ...  1.240706  19171.168444  0.999980
betas__2 -0.691408  0.311997  0.002154  ... -0.081642  18818.178323  0.999972
betas__3 -1.342901  0.338953  0.002298  ... -0.682267  19774.544152  0.999964
betas__4 -1.578479  0.412941  0.002856  ... -0.766116  20510.665307  1.000039
betas__5 -3.390476  1.020306  0.006719  ... -1.366569  19924.179007  1.000074

[6 rows x 7 columns]
WAIC_r(WAIC=470.5330024384129, WAIC_se=17.644125072856696, p_WAIC=5.839123546449564, var_warn=0)
LOO_r(LOO=470.53920018046125, LOO_se=17.644580587058424, p_LOO=5.842222417473749, shape_warn=0)
```

(Full disclosure: I edited out a bunch of `FutureWarning` messages from the output.)

The output related to the third model is

```
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 3 jobs)
NUTS: [psi, betas]
  axis=1, join_axes=[dforg.index])
              mean        sd  mc_error  ...  hpd_97.5         n_eff      Rhat
betas__0  0.002502  0.001278  0.000012  ...  0.005044  11189.477367  1.000043
betas__1  0.697889  0.354799  0.003536  ...  1.374610  11699.089031  0.999984
betas__2 -0.820867  0.372353  0.002906  ... -0.113082  12287.774661  0.999973
betas__3 -1.576939  0.404330  0.003665  ... -0.809346  11305.303087  0.999997
betas__4 -1.846225  0.479030  0.004939  ... -0.913815  10564.178241  1.000103
betas__5 -3.742761  1.161564  0.011286  ... -1.492667  11731.078882  0.999992

[6 rows x 7 columns]
/miniconda3/envs/crackedbassoon/lib/python3.7/site-packages/pymc3/stats.py:219: UserWarning: For one or more samples the posterior variance of the
        log predictive densities exceeds 0.4. This could be indication of
        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details
        
  """)
WAIC_r(WAIC=467.52213662957837, WAIC_se=17.274775074912508, p_WAIC=55.79775955528838, var_warn=1)
LOO_r(LOO=470.5709609001095, LOO_se=17.383117833712387, p_LOO=57.322171690553944, shape_warn=0)
```

(Again, I removed some warning messages.)

The point here is that the LVM converged and produced a sensible posterior distribution on
$$\boldsymbol{\beta}$$, which was very similar to the posterior from the Bayesian logistic regression. However, in this
example, using the LVM over the GZLM was not really worth the small amount of extra work. Sampling took longer under the
LVM due to the extra vector of random variables that needed to be sampled and the sampler took longer to converge, as
indicated by the smaller numbers of effective samples. Model-fit statistics were almost exactly the same for
both models (ignoring the warning message).

The major benefit of LVMs is that they generalize quite naturally to multivariate data. For example, consider the
multivariate logistic Bernoulli LVM,

$$\begin{equation}
\boldsymbol{\Psi}\sim\mathrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B}, \boldsymbol{\Theta}\right)\\
\boldsymbol{\Pi}=\mathrm{logistic}\left(\boldsymbol{\Psi}\right)\\
\boldsymbol{Y}\sim\mathrm{Bernoulli}\left(\boldsymbol{\Pi}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{\Theta}$$ is a correlation matrix (i.e., a covariance matrix with 1s down the diagonal). To my mind
at least, this model is more intuitive and understandable than the corresponding GZLMM.  

Unfortunately for frequentists, multivariate LVMs have truly hideous likelihood functions due to the multivariate nature
of $$\boldsymbol{\Psi}$$ (e.g., see Eq. 3 in [Coull and Agresti, 2000](#Coull2000a)), making fitting prohibitively
difficult. Again, however, Bayesian modeling in PyMC3 or Stan avoids such difficulties.

Below is a demonstration of a multivariate logistic Bernoulli LVM fitted to simulated data.

The above code is slightly more complicated than perhaps it should be because, at the time of writing, the PyMC3 class
for the Lewandowski–Kurowicka–Joe (LKJ) prior on correlation matrices [(Lewandowski et al., 2009)](#Lewandowski2009a)
has a bug in it. To get around this, we place an LKJ prior on  $$\boldsymbol{L}$$, a lower triangular matrix from the
Cholesky decomposition of a covariance matrix. We scale the covariance matrix to that its diagonals are all 1, making
it a correlation matrix.  

In conclusion, LVMs are in my opinion a potentially useful way to model multivariate data when the multivariate normal
assumption is violated. There are many statistical problems that could be framed in terms of multivariate LVMs, such as
factor analysis, structural equation modeling, and quantitative genetics. Of course, researchers have already considered the
consequences of non-normality and have proposed their own solutions in each of these areas, which are often implemented
in specialist statistical software. However, implementations tend to be specific rather than general; if the model
you want to fit deviates in some way from these implementations, you may have to code the model by hand. When combined
with Bayesian inference, the LVM approach is extremely flexible, allowing for unconventional or unique multivariate
models to be devised and fitted without the headache of deriving and coding difficult likelihood functions. 

So where's the rub? You may have already spotted it: Bayesian LVMs are sometimes very slow.
Sampling from the latent multivariate normal takes time, and due to the extra complexity, many MCMC samples may be
needed to obtain a good approximation of the joint posterior distribution. Thus, using LVMs is a trade-off between
theoretical simplicity and computational resources. In the future, I plan to explore how much LVMs benefit from being
run on GPUs.