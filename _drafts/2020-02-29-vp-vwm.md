---
layout: post
title: Exploring the power-law assumption of variable-precision visual short-term memory
date: 2019-08-27
has_code: true
has_math: true
has_comments: true
include_references: true
references:
 - Wixted2019a
 - Green1988a
 - Macmillan2005a
 - McNicol2005a
 - Wickens2002a
 - DeCarlo2010a
 - Kolmogorov1956a
 - Myung2003a
tags:
 - psychophysics
 - Python
 - statistics
---

_Visual short-term memory_ refers to the perceptual and cognitive processes that allow us
to remember the features of a visual scene for several seconds after it has disappeared
from view. Experts in the field often use the term _visual working memory_ instead
([Luck & Vogel, 2013](#Luck2013a), [Ma et al., 2014](#Ma2014a)) to connect 

_Visual working memory_ refers to the subcomponents of _working memory_ that are dedicated
to vision. Personally, I would refer to the call the processes discussed in this post

proceprefer the term _visual short-term memory_ because working memory
is supposed to involve both the passive storage and active manipulation of memories
[(Baddeley, 2012)](#Baddeley2012a) 




_Signal detection theory_ (SDT), or sometimes just _detection theory_, is a way of
understanding how an observer—usually a human in a psychological experiment—discriminates
between different categories of information. SDT plays a particularly important role in
the subfield of psychology known as _psychophysics_, which is concerned with the
relationships between physical stimuli and how they are perceived. Nowadays, SDT is one of
the most widely accepted and extensively used theories in all of psychology and
neuroscience.

This post provides a somewhat anachronistic introduction to SDT. I start by describing a
simple experiment. Next I describe the most common SDT model used to analyze data from
such an experiment. I next explain how the free parameters from this model are used to
generate predictions of trial outcomes. Finally, I rearrange those predictions so that
parameter estimates can be obtained from observed data.

This post eschews some of the concepts that appear in other treatments of SDT, such as
receiver-operating characteristic curves, likelihood ratios, and optimal decision-making.
This is because I don't believe that they are essential for typical use cases. If you are
looking for a complete formulation of SDT, the foundational textbook by [Green and Swets
(1988)](#Green1988a) is the way to go. Another commonly cited SDT reference is [Macmillan
and Creelman (2005)](#Macmillan2005a). Both of these books are quite dense and could be
difficult to follow for beginners, so for a gentler introduction, you might want to try
either [McNicol (2005)](#McNicol2005a) or [Wickens (2002)](#Wickens2002a). For a detailed
history of SDT, I recommend the recent article by [Wixted (2019)](#Wixted2019).

## Yes/no experiment

Imagine a participant, or *observer*, in a psychophysical experiment. The experiment
comprises $$n$$ trials. During each trial, the observer hears a sound. The sound is either
_noise_, or noise plus a _signal_. In SDT parlance, “noise” means any unwanted or
uninteresting information; in this case, the “noise” is literally [Gaussian white
noise](https://en.wikipedia.org/wiki/White_noise).  By contrast, “signal” means
interesting information; in this case, the signal is a [pure tone](pure-tones).

![](/assets/images/noise_toneplusnoise.svg)
*Two example waveforms. The left-hand waveform is Gaussian noise. The right-hand waveform
is Gaussian noise mixed with a pure tone—you can see the effect of mixing the noise and
signal together in its not-perfectly regular peaks.*

Whether the observer hears noise or noise plus signal on a particular trial is random.
Let $$X$$ denote a random variable that represents whether a particular trial contained a
signal. If $$X=0$$, the sound was noise. If $$X=1$$, the sound was noise plus signal.
Often both trial types are equally likely, but this does not have to be the case.
Importantly, however, the probabilities of $$X=0$$ and $$X=1$$—denoted by
$$\mathrm{P}\left(X=0\right)$$ and $$\mathrm{P}\left(X=1\right)$$, respectively—are set
by the experimenter and are therefore always known.

After hearing a sound on a given trial, the observer answers the question, “Did you hear
a tone?” The observer must respond on each trial. Let $$Y$$ denote a random variable that
represents the observer’s response. If $$Y=0$$, they responded “yes.” If $$Y=1$$, they
responded “no.”

This kind of experiment is called a _yes/no_ (YN) experiment. The name is quite
misleading, in my opinion, because the defining features of YN experiments aren’t the
instructions or response options. One could imagine an experiment where the instructions
and responses are worded quite differently but the perceptual and decision-making processes
required to complete the task are exactly the same. YN experiments are actually defined
by presenting one stimulus per trial and having the observer judge to which of two
classes the stimulus belongs (noise or noise plus signal). Unfortunately, the names of
other experiment designs under SDT are just as bad, as we shall see in future posts.

## Equal-variance Gaussian model

The simplest and most well-known SDT model for YN experiments is the _equal-variance
Gaussian_ (EVG) model. This model, like all SDT models, has a _perceptual component_ and a
_response component_. This separation of components is very helpful when trying to
understand SDT models ([DeCarlo, 2010](#DeCarlo2010a)). The perceptual component has to do
with making _observations_ and the response component has to do with applying _decision
rules_.

### Perceptual component

According to the perceptual component of the model, on each trial in a YN experiment,
the observer generates a single observation. Observations are continuous random variables.
All SDT models make assumptions about the statistical properties of observations—that is,
the shape of the [probability distributions](https://en.wikipedia.org/wiki/Probability_distribution)
they are drawn from—but they are generally agnostic about their physiological
implementation. (It is possible to augment SDT models by connecting observations to
physiology, such as the activity of neurons. This work is really cool, but beyond the
scope of this post.)

Let $$\Psi$$ denote a continuous random variable to represent the observation on a trial.
Depending on the trial, $$\Psi$$ is drawn from one of two distributions: the noise
distribution when $$X=0$$ and the noise-plus-signal distribution when $$X=1$$. The EVG
model assumes that these distributions are both [normal (or Gaussian)](https://en.m.wikipedia.org/wiki/Normal_distribution)
and have equal variances. The noise distribution is considered to be standard normal; that is,
it has zero mean and unit variance. The _signal-plus-noise_  distribution has an unknown
mean, denoted by $$d$$, and unit variance.

![](/assets/images/sdt-evg-perceptual.svg)
*Illustration of the perceptual component of the EVG model.*

One way to write out the perceptual component of the EVG model is

$$\begin{align}
\Psi&=dX + Z\\
Z&\sim\textrm{Normal}\left(0, 1\right)\textrm{,}
\end{align}$$

but another more useful way is 

$$\begin{align}
\Psi\mid{}X&=0\sim\textrm{Normal}\left(0, 1\right)\\
\Psi\mid{}X&=1\sim\textrm{Normal}\left(d, 1\right)\textrm{.}
\end{align}$$

The probability that $$\Psi$$ equals a particular value, $$\psi$$, on a given trial is

$$\begin{equation}
\textrm{P}\left(\Psi=\psi\right)=\varphi\left(\psi-dX\right)\textrm{,}
\end{equation}$$

or again more usefully in terms of [conditional probabilities](https://en.wikipedia.org/wiki/Conditional_probability),

$$\begin{align}
\textrm{P}\left(\Psi=\psi\mid{}X=0\right)&=\varphi\left(\psi\right)\tag{1a}\label{eq:1a}\\
\textrm{P}\left(\Psi=\psi\mid{}X=1\right)&=\varphi\left(\psi - d\right)\textrm{,}\tag{1b}\label{eq:1b}
\end{align}$$

where $$\varphi$$ denotes the probability density function of the standard normal
distribution,

$$\begin{equation}
\varphi\left(t\right)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}t^2}\textrm{.}
\end{equation}$$

The unknown mean of the noise-plus-signal distribution, $$d$$, is a *free parameter* in
the EVG model. This parameter represents the distance between the means of the two
conditional probability distributions of $$\Psi$$. If $$d$$ is large, the probability that
one distribution generated a particular value $$\psi$$ is high, while the probability that
the other distribution generated $$\psi$$ is low. If $$d$$ is small, these two
probabilities are similar. The parameter has a clear psychological interpretation, which
we will discuss later.

### Response component

Under the response component of the EVG model, the decision rule is very simple: the
observer responds “yes” if $$\psi$$ exceeds a certain value, denoted by $$k$$, and
“no” otherwise. This decision rule can be written as

$$\begin{align}
Y&=0\textrm{ if }\Psi\le{}k\\
Y&=1\textrm{ if }\Psi>k\textrm{.}
\end{align}$$

![](/assets/images/sdt-evg-response.svg)
*Illustration of the response component of the EVG model.*

(There is another formulation of the decision rule of the EVG model, in terms of likelihood
ratios. Indeed, this is how canonical sources define it ([Green & Swets, 1988](#Green1988a);
[Macmillan & Creelman, 2005](#Macmillan2005a)). The validity of the likelihood-ratio rule
has been debated over the years and as mentioned in the introduction, understanding this
rule is not essential for using the EVG model under most typical circumstances. I may return to
it in a future post.)

The unknown value $$k$$ is the second and last free parameter of the EVG model. If $$k$$
is small, the observer will be more likely to respond “yes” ($$Y=1$$) than “no” ($$Y=0$$),
all else being equal. Conversely, if $$k$$ is large, the observer will be more likely to
respond “no” than “yes.” There is one point, $$d/2$$, where "yes" and "no" responses are
equally likely, all else being equal. We will discuss the psychological interpretation of
this parameter later.

## Prediction of trial outcomes

Because values of $$\Psi$$ are unknowable, it is impossible to predict with certainty
how the observer will respond on a given trial. However, we can use the EVG model to
calculate the probabilities of the different outcomes.

There are four possible trial outcomes. The observer could make a *correct rejection*,
responding “no” on a noise trial; they could make a *miss*, responding “no” on a signal
trial; they could make a *false alarm*, responding “yes” on a noise trial; or they could
make a *hit*, responding “yes” on a signal trial. To summarize,

|         | $$X=0$$           | $$X=1$$ |
|---------|:-----------------:|:-------:|
| $$Y=0$$ | correct rejections | misses    |
| $$Y=1$$ | false alarms       | hits     |

We only need to concern ourselves with two of these outcomes. By convention, we choose
false alarms and hits.

### False-alarm rate

From the decision rule of the EVG model, it follows that

$$\begin{align}
\textrm{P}\left(Y=1\mid{}X=0\right)&=\textrm{P}\left(\Psi > k\mid{}X=0\right)\textrm{.}\end{align}$$

This the conditional probability of a false alarm, sometimes called the *false-alarm rate*,
denoted by $$f$$. 

![](/assets/images/sdt-evg-fa.svg)
*Shaded area is the false-alarm rate.*

The false-alarm rate is *not* the same as the [unconditional, or marginal, probability](https://en.wikipedia.org/wiki/Marginal_distribution)
of a false alarm, which is actually the joint probability of $$X=0$$ and $$Y=1$$.
The latter probability can be found by applying the axioms of probability
([Kolmogorov, 1956; p. 7](#Kolmogorov1956a)) to discover that the joint probability of two events,
$$A$$ and $$B$$ is equal to the conditional probability of $$A$$ given $$B$$ multiplied
by the marginal probability of
$$B$$,

$$\begin{equation}\mathrm{P}\left(A \cap B\right) = \mathrm{P}\left(A \mid B\right)\mathrm{P}\left(B\right)\textrm{.}\end{equation}$$

Thus,

$$\begin{equation}\mathrm{P}\left(X=0 \cap Y=1\right) =\\ \mathrm{P}\left(Y=1 \mid X=0\right)\mathrm{P}\left(X=0\right)\textrm{,}\end{equation}$$

where, as mentioned previously, $$\mathrm{P}\left(X=0\right)$$ is defined by the rules of the experiment and therefore always known.

From Equation $$\eqref{eq:1a}$$, it follows that

$$\begin{align}
f&=\textrm{P}\left(\Psi > k\mid{}X=0\right)\\
&=\int_{k}^{\infty}\varphi\left(\psi\right) \mathrm{d}\psi=\Phi\left(-k\right)\textrm{,}\tag{2a}\label{eq:2a}
\end{align}$$

where $$\Phi$$ is the cumulative distribution function of the standard normal distribution.
Thus, it is possible to obtain an observer's $$f$$ from their value of $$k$$. 

### Hit rate

From the decision rule and Equation $$\eqref{eq:1b}$$, the conditional probability of a
hit or *hit rate*, denoted by $$h$$, is

$$\begin{align}
h&=\textrm{P}\left(Y=1\mid{}X=1\right)\\
&=\textrm{P}\left(\Psi> k\mid{}X=1\right)\\
&=\int_{k-d}^{\infty}\varphi\left(\psi\right) \mathrm{d}\psi=\Phi\left(d-k\right)\textrm{.}\tag{2b}\label{eq:2b}\end{align}$$

![](/assets/images/sdt-evg-h.svg)
*Shaded area is the hit rate.*

## Sensitivity and criterion

By combining and rearranging Equations $$\eqref{eq:2a}$$ and $$\eqref{eq:2b}$$, we can
find equations for $$k$$ and $$d$$ in terms of $$f$$ and $$h$$. From Equation $$\eqref{eq:2a}$$,

$$\begin{align}k=-\Phi^{-1}\left(f\right)\textrm{,}\end{align}$$

where $$\Phi$$ is the inverse of the cumulative distribution function of the standard
normal distribution. From Equation $$\eqref{eq:2b}$$,

$$\begin{align}
d-k&=\Phi^{-1}\left(h\right)\\
d&=\Phi^{-1}\left(h\right)+k\\
d&=\Phi^{-1}\left(h\right)-\Phi^{-1}\left(f\right)\textrm{.}\\
\end{align}$$

The EVG model is more usually parameterized in terms of *sensitivity*, denoted by
$$d^\prime$$, and *criterion*, denoted by $$c$$. Sensitivity is the defined as the
standardized distance between the means of the noise and noise-plus-signal distributions.
Here, "standardized" means that the standard deviations of the two distributions are
pooled. Because both distributions have unit variance
under the EVG model, standardized difference is the same as raw difference, so

$$\begin{align}d^\prime&=d\\
&=\Phi^{-1}\left(h\right)-\Phi^{-1}\left(f\right)\textrm{.}\tag{3a}\label{eq:3a}\end{align}$$

Note that this is not the case for other SDT models, where the standard deviations of the
two distributions are not necessarily the same.

Criterion is defined as the distance of $$k$$ from the point where $$\Psi$$ is equally
likely under both models. Thus,

$$\begin{align}
c&=k-\frac{d}{2}\\
&=-\Phi^{-1}\left(f\right)-\frac{1}{2}\left[\Phi^{-1}\left(h\right)-\Phi^{-1}\left(f\right)\right]\\
&=-\frac{1}{2}\left[\Phi^{-1}\left(h\right)+\Phi^{-1}\left(f\right)\right]\textrm{.}\tag{3b}\label{eq:3b}
\end{align}$$

## Maximum likelihood estimation

Let $$N$$ and $$S$$ denote the respective counts of noise trials and noise-plus-signal
trials completed by an observer in an experiment. Also let $$F$$ denote the number of
observed false alarms and $$H$$ denote the number of hits. The maximum likelihood estimate
(MLE; [Myung, 2003](#Myung2003)) of the observer's false-alarm rate, denoted by $$\hat{f}$$,
is

$$\begin{align}
\hat{f}=\frac{F}{N}
\end{align}$$

and the MLE of their hit rate denoted by $$\hat{h}$$, is

$$\begin{align}
\hat{h}=\frac{H}{S}\textrm{.}
\end{align}$$

By swapping out $$f$$ for $$\hat{f}$$ and $$h$$ for $$\hat{h}$$ in the right-hand sides of
Equations $$\eqref{eq:3a}$$ and $$\eqref{eq:3b}$$, we obtain MLEs for sensitivity and criterion,
denoted by $$\hat{d^\prime}$$ and $$\hat{c}$$, respectively.

## Application to real data

I’ve written a Python script to run an experiment like the one described above. The experiment
contains 40 trials and should only take a minute or two to complete. Please check you
have installed all the dependencies into your active Python environment and adjust your
volume settings are instructed. Here is the code:

```python
{{ site.data.code.sdt-yn-experiment__py }}
```

Once complete, you will see your results summarized in a so-called *contingency table* and
estimates of sensitivity and criterion. When I ran this on myself, I got the following output:

```
Experiment done!
Here is your contingency table:
+-------+-------+-------+
|       | x = 0 | x = 1 |
+-------+-------+-------+
| y = 0 |   17  |   2   |
| y = 1 |   3   |   18  |
+-------+-------+-------+
Calculating SDT statistics ...
sensitivity (d') = 2.32
criterion (c) = -0.12
```

Obviously, if you run the experiment yourself, you’ll likely get different values.
The script throws out a warning message goes no further if any zeros appear in the
contingency table—I'll write more about what happens under these circumstances in a later
post. However, if all cells are non-zero, you’ll see something similar to what is shown
above.