---
layout: post
title: "Bayesian model of working-memory capacity"
date: 2017-02-19
categories:
- Bayesian
- Psychophysics
description:
image: https://sammosummo.github.io/images/memory-1948.jpg
image-sm: https://sammosummo.github.io/images/memory-1948-sm.jpg
image-description: "Memory by Rene Margitte (1948)"
---

Working memory (WM) is the name given to the memory system that stores information over short periods, which is strictly limited in terms of duration and capacity. In this post, I present a model for measuring WM capacity from psychophysical data using Bayesian hierarchical inference.

**(Disclaimer: Some of what appears below was taken from an earlier post on my previous blog. The code has been improved, and I have included a plug for one of my recent papers.)**

The nature of WM capacity limitation is a source of current debate. Some researchers argue that WM is *slots-based*[<sup>1</sup>], stating that we can remember up to a fixed number of discrete items at once. Others suggest that WM capacity is a finite *flexible resource*[<sup>2</sup>]. Personally, I lean more towards the latter view. However, item-based storage has long been the dominant view, and the maximum number of items stored in WM, denoted by \\( k \\), remains a popular dependent variable in the cognitive sciences.

[<sup>1</sup>]: https://doi.org/10.1016/j.tics.2013.06.006 "Luck, S.J., & Vogel, E.K. (2013). Visual working memory capacity: From psychophysics and neurobiology to individual differences. Trends in Cognitive Sciences, 17(8), 391–400."

[<sup>2</sup>]: https://doi.org/10.1038/nn.3655 "Ma, W. J., Hussain, M. & Bays, P. M. (2014). Changing concepts of working memory. Nature Neuroscience, 17, 347–356."

A simple way of measuring \\( k \\) is to apply a formula proposed by either Pashler[<sup>3</sup>] or Cowan[<sup>4</sup>] to the data from a change-detection task. On each trial in a change-detection task, the subject is presented with a stimulus comprising several distinct items (e.g., a visual array of objects) and after a few seconds, a second stimulus is presented. The second stimulus contains at least one item from the original stimulus (the probed item), which may or may not have changed in some way, and the subject indicates whether a change occurred. The choice of formula depends on whether the second stimulus also contains the remaining items from the original stimulus: if so, Pashler’s formula should be used; if it contains just the probed item, Cowan’s should be used.

[<sup>3</sup>]: https://www.ncbi.nlm.nih.gov/pubmed/3226885 "Pashler, H. (1988). Familiarity and visual change detection. Perception & Psychophysics, 44(4), 369–378."

[<sup>4</sup>]: https://www.ncbi.nlm.nih.gov/pubmed/11515286 "Cowan, N. (2001). The magic number 4 in short-term memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24(1), 87–114."

These formulae are easy to implement and are widely used in research, but there are several problems with them. First, using these formulae, \\( k \\) can only be calculated from trials with the same set size — experiments typically include trials with various set sizes, so estimates of \\( k \\) must be calculated separately for each set size and then combined, rather than calculating a single estimate from all the data. Second, since \\( k \\) can never exceed the set size, it is systematically underestimated whenever the set size is smaller than the true \\( k \\). Third, the formulae can yield negative values of \\( k \\), which are obviously impossible. Fourth, the formulae cannot be used to calculate \\( k \\) at all when performance is at ceiling or floor.

To remedy these issues, Morey[<sup>5</sup>] formalised a Bayesian hierarchical model for the measurement of WM capacity from change-detection tasks. The model largely deals with the problems listed above, and is much more efficient (it recovers parameters more accurately) than those formulae when there are few data. Morey provides his own software to fit the model[<sup>6</sup>].

[<sup>5</sup>]: https://doi.org/10.1016/j.jmp.2010.08.008 "Morey, R. D. (2011). A hierarchical Bayesian model for the measurement of working memory capacity. Journal of Mathematical Psychology, 55, 8–24"

[<sup>6</sup>]: https://dx.doi.org/10.3758%2Fs13428-011-0114-8 "Morey, R. D. & Morey, C. C. (2011). WoMMBAT: A user interface for hierarchical Bayesian estimation of working memory capacity. Behavior Research Methods, 43(4), 1044–1065."

I have implemented my version of this model using [PyMC3](http://docs.pymc.io/), a Python package for Bayesian modelling[<sup>7</sup>]. My version is not exactly the same as Morey’s. For example, my version uses PyMC3’s built-in inference methods, which are vastly more efficient than earlier ones (e.g., Metropolis) when sampling from hierarchical models. The code only applies to the Cowan style of change-detection tasks, although it would be easy to modify it to apply to Pashler-style tasks, or indeed any other variation, provided the decision rule can be specified.

[<sup>7</sup>]: https://doi.org/10.7717/peerj-cs.55 "Salvatier, J., Wiecki, T. V., & Fonnesbeck, C. (2016). Probabilistic programming in Python using PyMC3. PeerJ Computer Science, 2:e55."


The decision process
--------------------

The model assumes that, on a given trial, the subject may or may not suffer a lapse in attention. When a lapse occurs, the subject simply guesses same or different, with no regard for the stimulus. When the subject does not lapse, they perform the task properly. The probability of a non-lapse is denoted by \\( z \\).

On non-lapse trials, the subject remembers up to \\( k \\) items from the stimulus. If the set size, \\( M \\), is less than or equal to \\( k \\), all of them are remembered, but if \\( M \\) exceeds \\( k \\), a random selection of \\( k \\) items are remembered.

If the probed item was one of the ones remembered, the subject correctly responds same or different, depending on the type of trial. If the probed item was not remembered, the subject guesses, just like on a lapse trial. The probability that the subject guesses same is assumed to be fixed across both lapse and non-lapse trials (more on this later), and is denoted by \\( s \\).

With the decision process fully defined, we can now simulate the subject’s response on a given trial, provided we know the trial conditions, namely the set size and whether the correct response was same or different, and the values of the parameters \\(z \\), \\( k \\), and \\( s \\). The following Python function generates responses to single trials:

~~~ python
def trial(M, different, z, k, b):
    from numpy.random import uniform
    if uniform() > z:
        if uniform() < b:
            return True
        else:
            return False
    else:
        if uniform() < k / float(M):
            return different
        else:
            if uniform() < b:
                return True
        else:
            return False
~~~


Bayesian model (lower level)
----------------------------

To estimate \\(z \\), \\( k \\), and \\( s \\), we need to distinguish between *hits* and *false alarms* in a similar way to signal detection theory. We arbitrarily define a hit as a correct response on a different trial, and a false alarm as an incorrect response on a different trial. The number of hits, \\( H \\), and false alarms, \\(F \\) follow the binomial probability distributions

$$
H_{i_j} \sim \textrm{Binomial}\left(h_{i_j},D_{i_j}\right)
$$

and

$$
F_{i_j} \sim \textrm{Binomial}\left(f_{i_j},S_{i_j}\right)\textrm{,}
$$

respectively, where \\( h \\) is the hit probability, \\( D \\) is the number of different trials, \\( f \\) is the false-alarm probability, \\( S \\) is the number of same trials, \\( i \\) indexes the subject and/or condition, and \\( j \\) indexes the set size. (I assume that the experiment includes trials with more than one set size per subject/condition, which is almost always true.)

The relationships between \\( h \\) and \\( f \\) and the parameters of the decision process can be determined easily by considering the all the possible outcomes of a trial. The appropriate equations are

$$
h_{i_j}=\left(1-z_{i}\right)b_{i}+z_{i}q_{i_j}+z_{i}\left(1-q_{i_j}\right)b_i
$$

and

$$
f_{i_j}=\left(1-z_i\right)b_i+z_i\left(1-q_{i_j}\right)b_i\textrm{,}
$$

where

$$
q_{i_j}=\min\left(1,\frac{k_i}{M_{i_j}}\right)\textrm{.}
$$

It is generally a good idea to do inference on normally distributed variables. However, \\( k \\) cannot be normal, because negative values of \\( k \\) are impossible, and neither can \\( s \\) nor \\( z \\), because they are probabilities and must fall between 0 and 1. Therefore, we let

$$
\kappa_{i}=\max\left(k_i, 0\right)\textrm{,}\\
\varsigma_{i}=\textrm{logistic}\left(g_i\right)\textrm{,}
$$

and

$$
\zeta_{i}=\textrm{logistic}\left(z_i\right)\textrm{.}
$$

These comprise the first level of unknown random variables (stochastic variables) in the model. Next we assign priors to these variables.

$$
\kappa_{i}\sim\textrm{Normal}\left(\mu_{(\kappa)_i}, \sigma_{(\mu)}\right)\textrm{,}\\
\varsigma_{i}\sim\textrm{Normal}\left(\mu_{(\varsigma)_i}, \sigma_{(\varsigma)}\right)\textrm{,}
$$

and

$$
\zeta_{i}\sim\textrm{Normal}\left(\mu_{(\zeta)_i}, \sigma_{(\zeta)}\right)\textrm{.}
$$

Bayesian model (upper level)
----------------------------

Notice that the prior means on the stochastic variables (e.g., $$\mu_{(\kappa)_i}$$) have subscript \\( i \\)s, indicating that they are actually elements within vectors. These vectors are defined as follows:

$$
\vec{\mu_{\left(\kappa\right)}}=\begin{bmatrix} \mu_{(\kappa)_{i=0}} \\ \mu_{(\kappa)_{i=1}} \\ \vdots \\ \mu_{(\kappa)_{i=N}}\end{bmatrix}=\mathbf{X}_{\left(\kappa\right)}\vec{\beta_{\left(\kappa\right)}}\\
\vec{\mu_{\left(\varsigma\right)}}=\begin{bmatrix} \mu_{(\varsigma)_{i=0}} \\ \mu_{(\varsigma)_{i=1}} \\ \vdots \\ \mu_{(\varsigma)_{i=N}}\end{bmatrix}=\mathbf{X}_{\left(\varsigma\right)}\vec{\beta_{\left(\varsigma\right)}}\\
\vec{\mu_{\left(\zeta\right)}}=\begin{bmatrix} \mu_{(\zeta)_{i=0}} \\ \mu_{(\zeta)_{i=1}} \\ \vdots \\ \mu_{(\zeta)_{i=N}}\end{bmatrix}=\mathbf{X}_{\left(\zeta\right)}\vec{\beta_{\left(\zeta\right)}}
$$

In words, we have placed linear models on the prior means of our subject-level stochastic variables: \\( N\\) denotes the total number of subjects/conditions, \\( \mathbf{X} \\) denotes a design matrix, and \\( \vec{\beta} \\) denotes a vector of coefficients. The coefficients are stochastic variables, to which we assign vague priors:

$$
\beta_{\left(\kappa\right)}_{c_{\left(\kappa\right)}}
$$
