---
layout: post
title: Easy Bayesian generalization of mulitvariate linear models
date: 2019-10-10
has_comments: true
has_code: true
has_math: true
include_references: true
references:
 - Coull2000a
---
Consider the multivariate version of the general linear model (MGLM),

$$\begin{equation}
\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{B}+\boldsymbol{E}\textrm{,}
\end{equation}$$

where $$\boldsymbol{Y}$$ is an $$n\times{}m$$ matrix of data with rows indicating independent cases and columns
indicating different variables; $$\boldsymbol{X}$$ is an $$n\times{}k$$ matrix of covariates; $$\boldsymbol{B}$$ is an
$$m\times{}k$$ matrix of coefficients; and $$\boldsymbol{E}$$ is an $$n\times{}m$$ matrix of residuals following the
probability distribution

$$\begin{equation}
\boldsymbol{E}\sim\textrm{MvNormal}\left(0, \boldsymbol{\Sigma}\right)\textrm{.}
\end{equation}$$

For Bayesians, the following equivalent expression may feel more natural:

$$\begin{equation}
\boldsymbol{Y}\sim\textrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B},\boldsymbol{\Sigma}\right)\textrm{.}
\end{equation}$$

In the special case where $$\boldsymbol{\Sigma}$$ is a diagonal matrix, the  MGLM is equivalent to multiple
univariate GLMs (UGLMs), one on each dependent variable. The UGLM can be written as

$$\begin{equation}
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\\
\boldsymbol{e}\sim\textrm{Normal}\left(0, \sigma^2\right)\textrm{,}
\end{equation}$$

or equivalently,

$$\begin{equation}
\boldsymbol{y}\sim\textrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, \sigma^2\right)\textrm{.}
\end{equation}$$

UGLMs are simpler and more commonly used than MGLMs. So, if a researcher doesn't care about potential correlations
between dependent variables, or how such correlations might influence estimates of regression coefficients, they may
as well use several UGLMs.

Suppose that the data are categorical, ordinal, counts, or bounded between certain values. In such cases, the residuals
may not be well approximated by a normal distribution, making GLMs inappropriate.

When dealing with a single dependent variable, we can use a general*ized* linear model (GZLM). For example, for binary
data, we can perform _logistic regression_ using the following model:

$$\begin{equation}
\boldsymbol{\psi}=\boldsymbol{X}\boldsymbol{\beta}\\
\boldsymbol{p}=\mathrm{logistic}\left(\boldsymbol{\psi}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{p}\right)\textrm{.}
\end{equation}$$

In words, this model assumes that (1) the data are Bernoulli trials, and therefore that their likelihood is given by the
probability mass function of the Bernoulli distribution conditional on a set of success probabilities; (2) the vector of
logits of these probabilities are equal to a linear combination of covariates and coefficients. Another name for this
model could be logistic Bernoulli GZLM (LBGZLM).

This is all fine, but what do we do if we have a multivariate data set, the correlations between dependent variables are
important, _and_ the data aren't well approximated by a normal distribution?

This seems to be a difficult problem. For example, to my knowledge, no solution to modeling multiple correlated binary
dependent variables is as widely accepted as the LBGZLM is for modeling a single dependent variable.

One solution is to define a mixture distribution whose likelihood function is the probability density/mass function of
some distribution conditional on a set parameters, and assume that some transformation of the parameters follows a
multivariate normal distribution whose expectation is a linear combination of covariates and coefficients.

Importantly, this is not quite the same thing as a multivariate extension of a UGZLM. Consider the following model:

$$\begin{equation}
\boldsymbol{\psi}\sim\mathrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, 1\right)\\
\boldsymbol{p}=\mathrm{logistic}\left(\boldsymbol{\psi}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{p}\right)\textrm{.}
\end{equation}$$

Unlike the LBGZLM described earlier, $$\boldsymbol{\psi}$$ is not equal to $$\boldsymbol{X}\boldsymbol{\beta}$$. Rather,
$$\boldsymbol{\psi}$$ is a stochastic latent variable with expectation $$\boldsymbol{X}\boldsymbol{\beta}$$. The
variance of this latent variable must be fixed for the model to be identified, and unit variance is the obvious choice.
Hereafter, I refer to models of this kind as univariate latent-variable models (ULVMs), and the particular model
described above as the logistic Bernoulli ULVM (LBULVM).

Within the frequentist framework, fitting ULVMs is not as easy as fitting UGZLMs because the former have more
complicated likelihood functions. For example, whereas the likelihood function of the LBGZLM is simply the probability
mass function of the Bernoulli distribution after subsituting $$\boldsymbol{p}$$ for
$$\mathrm{logistic}\left(\boldsymbol{\psi}\right)$$, the likelihood function of the LBULVM involves marginalizing out
$$\boldsymbol{\psi}$$. (I've omitted this equation from this post because, frankly, I don't have the time
or patience to derive and typeset it.)

By contrast, within a Bayesian framework, fitting ULVMs is almost as easy as fitting UGLZMs using general software such
as PyMC3 or Stan. Because models in such software there is no need
to derive the marginalized likelihood functionâ€”we can estimate $$\boldsymbol{\psi}$$ with Markov Chain Monte Carlo
(MCMC), along with all the other variables in the model.

Here is a demonstration. The following script does three things: (1) fits a frequentist logistic regression to some data
using statsmodels; (2) fits a Bayesian version of the same model to the same data using PyMC3; and (3) fits a ULVM to
the same data using PyMC3. Notice that the code for (2) and (3) is not very different. The portion of the output related
to (1) is


The output related to (2) is

Notice how the marginal posterior means of $$\boldsymbol{\beta}$$ are very close to their maximum-likelihood estimates
(MLEs) from the frequentist model. Unsurprising, but reassuring.

The output related to (3) is

The marginal posteior means from this model are extremely similar to those from the Bayesian logistic regression.
Moreover, model-fit statistics are almost the same as well. This tells us something important: for all intents and
purposes, the UGZLM and ULVM are equivalent models.

Now let's discuss multivariate generalizations of the ULVMs (MLVMs). The multivariate analog of the ULVM presented
earlier is

$$\begin{equation}
\boldsymbol{\Psi}\sim\mathrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B}, \boldsymbol{\Theta}\right)\\
\boldsymbol{P}=\mathrm{logistic}\left(\boldsymbol{\Psi}\right)\\
\boldsymbol{Y}\sim\mathrm{Bernoulli}\left(\boldsymbol{P}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{\Theta}$$ is an $$m\times{}m$$ correlation matrix (i.e., a covariance matrix with 1s down the
diagonal). Fixing the diagonals is required for the same reason it was necessary to fix the variance of
$$\boldsymbol{\psi}$$ in the ULVM.

MLVMs have extremely complex likelihood functions because they involve intergrals that are multivariate in nature (e.g.,
see Eq. 3 in [Coull and Agresti, 2000](#Coull2000a)). Again, however, MCMC sampling of posteriors in Bayesian MLVMs via
PyMC3 or a similar software (e.g., Stan) does not require us to derive or code up this function explicitly.

Here is some example PyMC3 code to fit the MLVM defined above.

To my mind at least, MLVMs are a natural way to model the relationships between variables when the multivariate normal
assumption is clearly violated. There are many statistical problems that could be framed in terms of MVLMs, such as
as factor analysis, structural equation models, and multi-trait quantitative genetics models. There are, of course, 
numerous competing solutions for achieving the same goal in each of these examples, and many of them are computationally
simpler. In a frequentist framework, deriving and coding up the MLVM likelihood function may be prohibitively taxing. In
a Bayesian framework, however, one simply needs to define the model and sample. This flexibility could represent a major
advantage of MLVMs over other approaches, becuase it is straightforward to create a new model with a rare combination of
link function and likelihood that may not have been implemented in the specialized statistics program you typically use. However, a disadvantage of Bayesian MLVMs is
that they are slow. Sampling from the latent multivariate normal takes time, and due to the extra complexity, many MCMC
samples may needed to obtain a good approximation of the joint posterior distribution.

