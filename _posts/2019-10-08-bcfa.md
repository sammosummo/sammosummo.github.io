---
layout: post
title: Bayesian confirmatory factor analysis
date: 2020-11-08
has_comments: true
has_code: true
has_math: true
include_references: true
references:
 - Spearman1904a
 - Bollen1989a
 - Jöreskog1969a
 - Muthén2012a
---

Factor analysis tries to explain the relationships between observed variables in terms of a smaller number of unobserved
variables. Observed variables are sometimes called _manifest variables_, _indicators_, or _items_, whereas latent
variables are usually called _factors_. When researchers talk or write about factor analysis, they usually mean
_exploratory factor analysis_ (EFA; [Spearman, 1904](#Spearman1904a)), a collection of statistical techniques whose goal
is to generate a factor solution from the data. A second, less common form of factor analysis is _confirmatory factor
analysis_ (CFA; [Jöreskog, 1969](Jöreskog1969a)). In contrast to EFA, in CFA the factor solution is specified
beforehand. The factor solution may come from theory, other data, or perhaps an EFA of the same data. CFA is a
special case of _structural equation modeling_ (SEM; [Bollen, 1989](Bollen1989a)).

Here, I describe my first attempt to implement Bayesian CFA using PyMC3. This turned out to be trickier than I had
anticipated. At the time of writing, I’ve successfully fitted some basic models to a well-known data set, and found
similar results to those published in a previous study. These models are missing some common CFA features, but overall,
I think I’m on the right path and that my progress is worth sharing.

## Basic CFA model

The input to CFA is typically a matrix of raw data denoted by $$\boldsymbol{Y}$$ with shape $$n \times p$$, where $$n$$
is the number of cases and $$p$$ is the number of items. Before fitting the model, we must choose a value for $$m$$, the
number of factors. Finally, we must also choose which items load on (are related to) which factors. In basic CFA, the
data are assumed to be multivariate normal,


$$\begin{equation}
\boldsymbol{Y}\sim\mathrm{Normal}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{\mu}$$ is a $$p$$-length vector of means and $$\boldsymbol{\Sigma}$$ is an $$n \times p$$
covariance matrix. I haven't figured out the derivations myself yet, but according to authoritative sources, these
variables are given by

$$\begin{align}
\boldsymbol{\mu}=\boldsymbol{\nu}+\boldsymbol{\Lambda}\boldsymbol{\alpha}\textrm{,}\tag{1}\label{1}
\end{align}$$

where $$\boldsymbol{\mu}$$ is a $$p$$-length vector of item intercepts, $$\boldsymbol{\Lambda}$$ is a $$p \times m$$
matrix containing factor loadings, and $$\boldsymbol{\alpha}$$ is a $$m$$-length vector of factor intercepts; and

$$\begin{align}
\boldsymbol{\Sigma}=\boldsymbol{\Lambda}\boldsymbol{\Psi}\boldsymbol{\Lambda}^\mathrm{T}+\boldsymbol{\Theta}{,}
\tag{2}\label{2}
\end{align}$$

where $$\boldsymbol{\Psi}$$ is an $$m \times m$$ factor covariance matrix and $$\boldsymbol{\Theta}$$ is a
$$p \times p$$ item covariance matrix.

At first glance, constructing a Bayesian version of this model may seem like a straightforward task: define appropriate
priors for $$\boldsymbol{\nu}$$, $$\boldsymbol{\Lambda}$$, $$\boldsymbol{\alpha}$$, $$\boldsymbol{\Psi}$$, and
$$\boldsymbol{\Theta}$$; code up Equations $$\ref{1}$$ and $$\ref{2}$$; define the likelihood function; and sample
using Markov chain Monte Carlo (MCMC). However, there are a number of tricky issues that come up during this process,
described below.

### Issue \#1: Prior on $$\boldsymbol{\Lambda}$$

In frequentist CFA, $$\boldsymbol{\Lambda}$$ is always a sparse matrix; in other words, some elements within
$$\boldsymbol{\Lambda}$$ are free parameters, and all remaining elements are 0. The choice of which elements are free
and which are 0 defines the factor structure. If $$\lambda_{(i,j)}$$ is a free parameter, it is assumed that the $$i$$th
item loads on the $$j$$th factor.

There are practical limitations on how many free parameters $$\boldsymbol{\Lambda}$$ may contain in frequentist CFA. If
every element of $$\boldsymbol{\Lambda}$$ is free, the model is not
[_identified_](https://en.wikipedia.org/wiki/Identifiability), meaning that there is no unique maximum-likelihood
solution because there are too many free parameters. Hopefully, it also makes intuitive sense why you wouldn't want to
do this: if every item loads on every factor, what makes the factors different from one another?

While identifiability is not strictly an issue for Bayesian models, we still need to give $$\boldsymbol{\Lambda}$$ some
structure, otherwise our MCMC algorithm will not
[converge](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo#Convergence). The Bayesian way to impart such
structure is via informative priors.

As [Muthén and Asparouhov (2012)](#Muthén2012a) point out, the frequentist approach of assigning values of 0 to some
elements of $$\boldsymbol{\Lambda}$$ is analogous to assigning each of them a normal prior with mean and variance of 0
in a Bayesian model. Those authors go on to argue that such a prior could be considered to be _too_ informative: is it
realistic to assume that a given item does not load on a given factor at all? An alternative approach is to assign them
small but nonzero prior variances. This way, loadings between all items and all factors will be estimated, but the
so-called _cross-loadings_ will be smaller than the hypothesized loadings. However, a new question naturally arises:
how small should cross-loading prior variances be? Muthén and Asparouhov provide some recommendations, but the best
choice may not be obvious. I'll return to this later. 

### Issue \#2: Actually building $$\boldsymbol{\Lambda}$$  

If we follow [Muthén and Asparouhov (2012)](#Muthén2012a) and define $$\boldsymbol{\Lambda}$$ as a matrix of random
variables with non-zero variances, construction of the corresponding object in PyMC3 is not very difficult. However,
PyMC3 balks when the prior variance of a random variable is set to exactly 0. Unsurprising, but this makes it a bit
annoying to construct $$\boldsymbol{\Lambda}$$, because it requires using Theano, PyMC3's backend for computation, to
create a tensor that contains some variables and some constants. I tried various methods of doing this, and each of them
caused one or more bugs in my code, such as preventing PyMC3 from selecting the correct MCMC sampler, or causing
sampling to slow down markedly. Futhermore, Theano is unlikely to remain the computational backend of choice in future
versions of PyMC, so it would be better to work in the abstracted PyMC3 layer if possible.

My solution to this minor issue was to reparameterize the basic CFA model such that

$$\begin{align}                                                                                                
\lambda_{(i,j)}= \phi_{(i,j)}\sqrt{a_{(i,j)}}                                                                                     
\end{align}$$

where $$\boldsymbol{\Phi}$$ is a $$p \times m$$ matrix of
[independent and identically distributed random variables](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)
(i.e., all elements have the same prior variance), and $$\boldsymbol{A}$$ is a $$p \times m$$ matrix containing the
prior variances on elements within $$\boldsymbol{\Lambda}$$, which are specified by the user.

### Issue \#3: Prior on $$\boldsymbol{\Psi}$$

We run into a second identifiability issue if neither the variance of each factor nor at least one loading per factor is
fixed. I feel it makes the most sense to fix the factor variances. This makes $$\boldsymbol{\Psi}$$ a correlation matrix
rather than a covariance matrix.

The Inverse-Wishart distribution is defined on real-valued positive-definite matrices and is the conjugate prior for the
covariance matrix of a multivariate normal distribution. However, since Inverse-Wishart has some problematic features
([Alvarez, Niemi, & Simpson, 2014](#Alvarez2014a)) and we don't care about conjugate priors much these days, a better
choice is the LKJ prior ([Lewandowski, Kurowicka, & Joe, 2009](#Lewandowski2009)).

Unfortunately, while PyMC3 contains the `LKJCorr` distribution, it is currently broken, so we must construct an
LKJ prior on $$\boldsymbol{\Psi}$$ indirectly.