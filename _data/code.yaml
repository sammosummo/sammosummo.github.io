zotero__py: |
  """Download my zotero library and convert it to YAML.
  
  """
  from pyzotero import zotero
  
  
  def get_ids():
      """Read file containing ID and API key.
  
      """
      return [l.strip() for l in open("../../_data/zotero.txt").readlines()]
  
  
  def download_refs():
      """Download my enitre zotero library.
  
      """
      library_id, api_key = get_ids()
      zot = zotero.Zotero(library_id, "user", api_key)
      results = zot.everything(zot.top())
      for item in results:
          print(item["data"].keys())
  
  
  if __name__ == "__main__":
      download_refs()
bsem__py: |
  """Example of Bayesian confirmatory factor analysis in PyMC3.
  
  """
  import numpy as np
  import pandas as pd
  import pymc3 as pm
  import theano.tensor as tt
  import matplotlib.pyplot as plt
  from os.path import exists
  
  from matplotlib import rcParams
  from pymc3.math import matrix_dot, matrix_inverse
  from tabulate import tabulate
  
  
  def bsem(
      items,
      factors,
      paths,
      beta="estimate",
      nu_sd=2.5,
      alpha_sd=2.5,
      d_beta=2.5,
      corr_items=True,
      corr_factors=False,
      g_eta=100,
      l_eta=1,
      beta_beta=1,
  ):
      r"""Constructs Bayesian SEM.
  
      Args:
          items (np.array): Array of item data.
          factors (np.array): Factor design.
          paths (np.array): Array of directed factor paths.
          beta (:obj:`float` or `'estimate'`, optional): Standard deviation of normal
              prior on cross loadings. If `'estimate'`,  beta is estimated from the data.
          nu_sd (:obj:`float`, optional): Standard deviation of normal prior on item
              intercepts.
          alpha_sd (:obj:`float`, optional): Standard deviation of normal prior on factor
              intercepts.
          d_beta (:obj:`float`, optional): Scale parameter of half-Cauchy prior on factor
              standard deviation.
          corr_factors (:obj:`bool`, optional): Allow correlated factors.
          corr_items (:obj:`bool`, optional): Allow correlated items.
          g_eta (:obj:`float`, optional): Shape parameter of LKJ prior on residual item
              correlation matrix.
          l_eta (:obj:`float`, optional): Shape parameter of LKJ prior on factor
              correlation matrix.
          beta_beta (:obj:`float`, optional): Beta parameter of beta prior on beta.
  
      Returns:
  
          None: Places model in context.
  
      """
      # get numbers of cases, items, and factors
      n, p = items.shape
      p_, m = factors.shape
      assert p == p_, "Mismatch between data and factor-loading matrices"
      assert paths.shape == (m, m), "Paths matrix has wrong shape"
      I = tt.eye(m, m)
  
      # place priors on item and factor intercepts
      nu = pm.Normal(name=r"$\nu$", mu=0, sd=nu_sd, shape=p, testval=items.mean(axis=0))
      alpha = pm.Normal(name=r"$\alpha$", mu=0, sd=alpha_sd, shape=m, testval=np.zeros(m))
  
      # place priors on unscaled factor loadings
      Phi = pm.Normal(name=r"$\Phi$", mu=0, sd=1, shape=factors.shape, testval=factors)
  
      # place priors on paths
      B = tt.zeros(paths.shape)
      npths = np.sum(paths, axis=None)
      if npths > 0:
          b = pm.Normal(name=r"$b$", mu=0, sd=1, shape=npths, testval=np.ones(npths))
          # create the paths matrix
          k = 0
          for i in range(m):
              for j in range(m):
                  if paths[i, j] == 1:
                      B = tt.set_subtensor(B[i, j], b[k])
                      k += 1
      B = pm.Deterministic("$B$", B)
  
      # create masking matrix for factor loadings
      if isinstance(beta, str):
          assert beta == "estimate", f"Don't know what to do with '{beta}'"
          beta = pm.Beta(name=r"$\beta$", alpha=1, beta=beta_beta, testval=0.1)
      M = (1 - np.asarray(factors)) * beta + np.asarray(factors)
  
      # create scaled factor loadings
      Lambda = pm.Deterministic(r"$\Lambda$", Phi * M)
  
      # determine item means
      mu = nu + matrix_dot(Lambda, alpha)
  
      # place priors on item standard deviations
      D = pm.HalfCauchy(name=r"$D$", beta=d_beta, shape=p, testval=items.std(axis=0))
  
      # place priors on item correlations
      f = pm.Lognormal.dist(sd=0.25)
      if not corr_items:
          Omega = np.eye(p)
      else:
          G = pm.LKJCholeskyCov(name=r"$G$", eta=g_eta, n=p, sd_dist=f)
          ch1 = pm.expand_packed_triangular(p, G, lower=True)
          K = tt.dot(ch1, ch1.T)
          sd1 = tt.sqrt(tt.diag(K))
          Omega = pm.Deterministic(r"$\Omega$", K / sd1[:, None] / sd1[None, :])
  
      # determine residual item variances and covariances
      Theta = pm.Deterministic(r"$\Theta$", D[None, :] * Omega * D[:, None])
  
      # place priors on factor correlations
      if not corr_factors:
          Psi = np.eye(m)
      else:
          L = pm.LKJCholeskyCov(name=r"$L$", eta=l_eta, n=m, sd_dist=f)
          ch = pm.expand_packed_triangular(m, L, lower=True)
          Gamma = tt.dot(ch, ch.T)
          sd = tt.sqrt(tt.diag(Gamma))
          Psi = pm.Deterministic(r"$\Psi$", Gamma / sd[:, None] / sd[None, :])
  
      # determine variances and covariances of items
      A = matrix_inverse(I - B)
      C = matrix_inverse(I - B.T)
      Sigma = matrix_dot(Lambda, A, Psi, C, Lambda.T) + Theta
  
      # place priors on observations
      pm.MvNormal(name="$Y$", mu=mu, cov=Sigma, observed=items, shape=items.shape)
  
  
  def main():
  
      # load the data
      df = pd.read_csv("../../assets/data/HS.csv", index_col=0)
  
      # define items to keep
      item_names = [
          "visual",
          "cubes",
          "paper",
          "flags",
          "general",
          "paragrap",
          "sentence",
          "wordc",
          "wordm",
          "addition",
          "code",
          "counting",
          "straight",
          "wordr",
          "numberr",
          "figurer",
          "object",
          "numberf",
          "figurew",
      ]
  
      # define the factor structure
      factors = np.array(
          [
              [1, 0, 0, 0, 0],
              [1, 0, 0, 0, 0],
              [1, 0, 0, 0, 0],
              [1, 0, 0, 0, 0],
              [0, 1, 0, 0, 0],
              [0, 1, 0, 0, 0],
              [0, 1, 0, 0, 0],
              [0, 1, 0, 0, 0],
              [0, 1, 0, 0, 0],
              [0, 0, 1, 0, 0],
              [0, 0, 1, 0, 0],
              [0, 0, 1, 0, 0],
              [0, 0, 1, 0, 0],
              [0, 0, 0, 1, 0],
              [0, 0, 0, 1, 0],
              [0, 0, 0, 1, 0],
              [0, 0, 0, 1, 0],
              [0, 0, 0, 1, 0],
              [0, 0, 0, 1, 0],
          ]
      )
  
      paths = np.array(
          [
              [0, 0, 0, 0, 1],
              [0, 0, 0, 0, 1],
              [0, 0, 0, 0, 1],
              [0, 0, 0, 0, 1],
              [0, 0, 0, 0, 0],
          ]
      )
  
      # iterate over the two schools
      for school, sdf in df.groupby("school"):
  
          # define the path to save results
          f = f"../data/{school}"
  
          # select the 19 commonly used variables
          items = sdf[item_names]
  
          # for numerical convenience, standardize the data
          items = (items - items.mean()) / items.std()
  
          with pm.Model():
  
              # construct the model
              bsem(items, factors, paths)
  
              if not exists(f):
  
                  # sample and save
                  trace = pm.sample(chains=2)  # 19000, tune=1000,
                  pm.save_trace(trace, f)
                  pm.traceplot(trace, compact=True)
                  rcParams["font.size"] = 14
                  plt.savefig(f"{f}/traceplot.png")
  
              else:
  
                  trace = pm.load_trace(f)
  
          # create a nice summary table
          loadings = pd.DataFrame(
              trace[r"$\Lambda$"].mean(axis=0).round(3),
              index=[v.title() for v in item_names],
              columns=["Spatial", "Verbal", "Speed", "Memory", "g"],
          )
          loadings.to_csv(f"{f}/loadings.csv")
  
          # correlations = pd.DataFrame(
          #     trace[r"$\Psi$"].mean(axis=0).round(3),
          #     index=["Spatial", "Verbal", "Speed", "Memory", "g"],
          #     columns=["Spatial", "Verbal", "Speed", "Memory", "g"],
          # )
          # correlations.to_csv(f"{f}/factor_correlations.csv")
  
          correlations = pd.DataFrame(
              trace[r"$B$"].mean(axis=0).round(3),
              index=["Spatial", "Verbal", "Speed", "Memory", "g"],
              columns=["Spatial", "Verbal", "Speed", "Memory", "g"],
          )
          correlations.to_csv(f"{f}/factor_paths.csv")
  
          correlations = pd.DataFrame(
              trace[r"$\Omega$"].mean(axis=0).round(3),
              index=item_names,
              columns=item_names,
          )
          correlations.to_csv(f"{f}/item_correlations.csv")
  
  
  if __name__ == "__main__":
      main()
run__py: |
  """Run this script before committing.
  
  """
  from code2yaml import code2yaml
  from make_my_bib import make_my_bib
  
  
  if __name__ == "__main__":
  
      code2yaml()
      make_my_bib()
phist__py: |
  """How to create a partially filled histogram.
  
  """
  import matplotlib.pyplot as plt
  from matplotlib import rcParams as defaults
  from numpy.random import normal
  
  defaults["lines.linewidth"] = 2
  defaults["font.size"] = 14
  
  
  if __name__ == "__main__":
  
      y = normal(0, 1, 1000)  # generate some data
      fig, ax = plt.subplots(1, 1, constrained_layout=True)
      ns, bins, _ = ax.hist(y, 20, histtype="step")  # plot the unfilled histogram
      start, stop = [-1.96, 1.96]  # range to fill between
  
      for n, l, r in zip(ns, bins, bins[1:]):
  
          if l > start:
              if r < stop:
                  # these bins fall completely within the range
                  ax.fill_between([l, r], 0, [n, n], alpha=0.5)
              elif l < stop < r:
                  ax.fill_between([l, stop], 0, [n, n], alpha=0.5)  # partial fill
          elif l < start < r:
              ax.fill_between([start, r], 0, [n, n], alpha=0.5)  # partial fill
  
      plt.savefig(f"../../assets/images/phist.svg", bbox_inches=0, transparent=True)
bcfa__py: |
  """Example of Bayesian confirmatory factor analysis in PyMC3.
  
  """
  import numpy as np
  import pandas as pd
  import pymc3 as pm
  import theano.tensor as tt
  import matplotlib.pyplot as plt
  from os.path import exists
  
  from matplotlib import rcParams
  from pymc3.math import matrix_dot
  from tabulate import tabulate
  
  
  def bcfa(
      items,
      factors,
      beta="estimate",
      nu_sd=2.5,
      alpha_sd=2.5,
      d_beta=2.5,
      corr_items=True,
      corr_factors=True,
      g_eta=100,
      l_eta=1,
      beta_beta=1,
  ):
      r"""Constructs a Bayesian CFA model.
  
      Args:
          items (np.array): Array of item data.
          factors (np.array): Factor design matrix.
          beta (:obj:`float` or `'estimate'`, optional): Standard deviation of normal
              prior on cross loadings. If `'estimate'`,  beta is estimated from the data.
          nu_sd (:obj:`float`, optional): Standard deviation of normal prior on item
              intercepts.
          alpha_sd (:obj:`float`, optional): Standard deviation of normal prior on factor
              intercepts.
          d_beta (:obj:`float`, optional): Scale parameter of half-Cauchy prior on factor
              standard deviation.
          corr_factors (:obj:`bool`, optional): Allow correlated factors.
          corr_items (:obj:`bool`, optional): Allow correlated items.
          g_eta (:obj:`float`, optional): Shape parameter of LKJ prior on residual item
              correlation matrix.
          l_eta (:obj:`float`, optional): Shape parameter of LKJ prior on factor
              correlation matrix.
          beta_beta (:obj:`float`, optional): Beta parameter of beta prior on beta.
  
      Returns:
  
          None: Places model in context.
  
      """
      # get numbers of cases, items, and factors
      n, p = items.shape
      p_, m = factors.shape
      assert p == p_, "Mismatch between data and factor-loading matrices"
  
      # place priors on item and factor intercepts
      nu = pm.Normal(name=r"$\nu$", mu=0, sd=nu_sd, shape=p, testval=items.mean(axis=0))
      alpha = pm.Normal(name=r"$\alpha$", mu=0, sd=alpha_sd, shape=m, testval=np.zeros(m))
  
      # place priors on unscaled factor loadings
      Phi = pm.Normal(name=r"$\Phi$", mu=0, sd=1, shape=factors.shape, testval=factors)
  
      # create masking matrix for factor loadings
      if isinstance(beta, str):
          assert beta == "estimate", f"Don't know what to do with '{beta}'"
          beta = pm.Beta(name=r"$\beta$", alpha=1, beta=beta_beta, testval=0.1)
      M = (1 - np.asarray(factors)) * beta + np.asarray(factors)
  
      # create scaled factor loadings
      Lambda = pm.Deterministic(r"$\Lambda$", Phi * M)
  
      # determine item means
      mu = nu + matrix_dot(Lambda, alpha)
  
      # place priors on item standard deviations
      D = pm.HalfCauchy(name=r"$D$", beta=d_beta, shape=p, testval=items.std(axis=0))
  
      # place priors on item correlations
      f = pm.Lognormal.dist(sd=0.25)
      if not corr_items:
          Omega = np.eye(p)
      else:
          G = pm.LKJCholeskyCov(name=r"$G$", eta=g_eta, n=p, sd_dist=f)
          ch1 = pm.expand_packed_triangular(p, G, lower=True)
          K = tt.dot(ch1, ch1.T)
          sd1 = tt.sqrt(tt.diag(K))
          Omega = pm.Deterministic(r"$\Omega$", K / sd1[:, None] / sd1[None, :])
  
      # determine residual item variances and covariances
      Theta = pm.Deterministic(r"$\Theta$", D[None, :] * Omega * D[:, None])
  
      # place priors on factor correlations
      if not corr_factors:
          Psi = np.eye(m)
      else:
          L = pm.LKJCholeskyCov(name=r"$L$", eta=l_eta, n=m, sd_dist=f)
          ch = pm.expand_packed_triangular(m, L, lower=True)
          Gamma = tt.dot(ch, ch.T)
          sd = tt.sqrt(tt.diag(Gamma))
          Psi = pm.Deterministic(r"$\Psi$", Gamma / sd[:, None] / sd[None, :])
  
      # determine variances and covariances of items
      Sigma = matrix_dot(Lambda, Psi, Lambda.T) + Theta
  
      # place priors on observations
      pm.MvNormal(name="$Y$", mu=mu, cov=Sigma, observed=items, shape=items.shape)
  
  
  def main():
  
      # load the data
      df = pd.read_csv("../../assets/data/HS.csv", index_col=0)
  
      # define items to keep
      item_names = [
          "visual",
          "cubes",
          "paper",
          "flags",
          "general",
          "paragrap",
          "sentence",
          "wordc",
          "wordm",
          "addition",
          "code",
          "counting",
          "straight",
          "wordr",
          "numberr",
          "figurer",
          "object",
          "numberf",
          "figurew",
      ]
  
      # define the factor structure
      factors = np.array(
          [
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
          ]
      )
  
      # iterate over the two schools
      for school, sdf in df.groupby("school"):
  
          # define the path to save results
          f = f"../data/{school}"
  
          # select the 19 commonly used variables
          items = sdf[item_names]
  
          # for numerical convenience, standardize the data
          items = (items - items.mean()) / items.std()
  
          with pm.Model():
  
              # construct the model
              bcfa(items, factors)
  
              if not exists(f):
  
                  # sample and save
                  trace = pm.sample(19000, tune=1000, chains=2)
                  pm.save_trace(trace, f)
                  pm.traceplot(trace, compact=True)
                  rcParams["font.size"] = 14
                  plt.savefig(f"{f}/traceplot.png")
  
              else:
  
                  trace = pm.load_trace(f)
  
          # create a nice summary table
          loadings = pd.DataFrame(
              trace[r"$\Lambda$"].mean(axis=0).round(3),
              index=[v.title() for v in item_names],
              columns=["Spatial", "Verbal", "Speed", "Memory"],
          )
          loadings.to_csv(f"{f}/loadings.csv")
          print(tabulate(loadings, tablefmt="pipe", headers="keys"))
          correlations = pd.DataFrame(
              trace[r"$\Psi$"].mean(axis=0).round(3),
              index=["Spatial", "Verbal", "Speed", "Memory"],
              columns=["Spatial", "Verbal", "Speed", "Memory"],
          )
          correlations.to_csv(f"{f}/factor_correlations.csv")
          print("\n")
          print(tabulate(correlations, tablefmt="pipe", headers="keys"))
          correlations = pd.DataFrame(
              trace[r"$\Omega$"].mean(axis=0).round(3),
              index=item_names,
              columns=item_names,
          )
          correlations.to_csv(f"{f}/item_correlations.csv")
  
  
  if __name__ == "__main__":
      main()
fmt_val_latex__py: |
  """Format a number for inclusion in LaTeX.
  
  """
  from scipy.stats import cauchy
  
  
  def format_for_latex(x, p=3):
      """Convert a float to a LaTeX-formatted string displaying the value to p significant
      digits and in standard form.
  
      Args:
          x (float): Value.
          p (:obj:`int`, optional): Number of significant digits. Default is 3.
  
      Return:
          s (str): Formatted value.
      """
  
      def _f(x, p):
          s = "{:g}".format(float("{:.{p}g}".format(x, p=3)))
          n = p - len(s.replace(".", "").replace("-", "").lstrip("0"))
          s += "0" * n
          return s
  
      if abs(x) < 10 ** -p or abs(x) > 10 ** (p + 1):
          a, b = str("%e" % x).split("e")
          return "$%s$" % r"%s \times 10^{%i}" % (_f(float(a), p), int(b))
  
      return "$%s$" % _f(x, p)
  
  
  def test():
      X = cauchy.rvs(size=100000)
      for x in X:
          print(x, format_for_latex(x))
  
  
  if __name__ == "__main__":
      test()
neals-funnel-b__py: |
  """Generate data and sample from Neal's funnel distribution.
  
  """
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib import rcParams
  import pymc3 as pm
  from scipy.stats import norm
  
  
  def main():
  
      with pm.Model():
  
          # set up figure
          fs = rcParams["figure.figsize"]
          rcParams["figure.figsize"] = (fs[0], fs[0] / 2)
          rcParams["lines.linewidth"] = 2
          rcParams["font.size"] = 14
  
          # simulate data
          np.random.seed(0)
          k = 9
          n = 10000
          v = norm.rvs(0, 3, n)
          x = norm.rvs(0, np.exp(v / 2), (k, n))
  
          # plot simulated data
          fig, axes = plt.subplots(
              1, 2, constrained_layout=True, sharex=True, sharey=True
          )
          ax = axes[0]
          ax.scatter(x[0], v, marker=".", alpha=0.05, rasterized=True)
          ax.set_xlim(-20, 20)
          ax.set_ylim(-9, 9)
          ax.set_xlabel("$x_0$")
          ax.set_ylabel("$v$")
  
          # set up model
          v_ = pm.Normal("v", mu=0, sd=3)
          x_ = pm.Normal("x", mu=0, sd=pm.math.exp(v_ / 2), shape=k)
  
          # sample and save samples
          trace = pm.sample(n, chains=1)
          v_samples = trace["v"][:]
          x_samples = trace["x"][:].T
  
          # plot samples
          ax = axes[1]
          ax.scatter(
              x_samples[0], v_samples, marker=".", alpha=0.05, rasterized=True, color="r"
          )
          ax.set_xlabel("$x_0$")
  
          # save
          plt.savefig("../images/neals-funnel-b.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
nbib2yaml__py: |
  """Convert .nbib files to .yaml files.
  
  """
  import copy
  import pickle
  import os
  import re
  import string
  from pubmed_lookup import PubMedLookup
  from pubmed_lookup import Publication
  
  
  def main():
  
      p = "../../assets/nbibs"
      nbibs = [os.path.join(p, f) for f in os.listdir(p) if ".nbib" in f]
      papers = []
      paper = {}
  
      for f in nbibs:
          print(f"reading {f}")
          with open(f) as fp:
              for l in fp:
                  l = l.rstrip()
                  if l == "":
                      # blank line, new paper
                      if "id" in paper:
                          # save old paper
                          papers.append(copy.copy(paper))
                      # blank paper
                      paper = {}
                  elif l[4] == "-":
                      name, data = l.split("-", 1)
                      name = name.rstrip()
                      data = data.strip()
                      if name == "ID":
                          # non-pubmed record
                          paper["id"] = data
                          authors_from_pubmed = None
                      elif name == "PMID":
                          # pubmed record
                          paper["id"] = data
                          paper["pmid"] = data
                          _f = os.path.join(p, f"{data}.pkl")
                          if os.path.exists(_f):
                              # already saved the author list
                              with open(_f, "rb") as fp2:
                                  authors_from_pubmed = pickle.load(fp2)
                          else:
                              # getting the author list with special chars
                              print(f"looking up {data}")
                              url = f"http://www.ncbi.nlm.nih.gov/pubmed/{data}"
                              lookup = PubMedLookup(url, "")
                              publication = Publication(lookup, resolve_doi=False)
                              authors = [a.split() for a in publication._author_list]
                              x = lambda a: all(
                                  (a == a.upper(), "ENIGMA" not in a, "CNV" not in a)
                              )
                              y = lambda a: ", " + ". ".join(a) + "." if x(a) else a
                              authors = [[y(a) for a in b] for b in authors]
                              authors = [" ".join(a).replace(" ,", ",") for a in authors]
                              print("authors ->", authors)
                              with open(_f, "wb") as fp2:
                                  pickle.dump(authors, fp2)
                              authors_from_pubmed = copy.copy(authors)
                          paper["authors"] = authors_from_pubmed
                      elif name == "VI":
                          paper["volume"] = data
                      elif name == "IP":
                          paper["issue"] = data
                      elif name == "DP":
                          paper["year"] = data.split()[0]
                          paper["sort"] = "%s" % paper["year"]
                          try:
                              paper["month"] = data.split()[1]
                              month_number = dict(
                                  zip(
                                      [
                                          "Jan",
                                          "Feb",
                                          "Mar",
                                          "Apr",
                                          "May",
                                          "Jun",
                                          "Jul",
                                          "Aug",
                                          "Sep",
                                          "Oct",
                                          "Nov",
                                          "Dec",
                                      ],
                                      range(12),
                                  )
                              )[paper["month"]]
                              paper["sort"] = "%s-%02d" % (
                                  paper["year"],
                                  month_number + 1,
                              )
                              paper["month"] = str(paper["month"])
                          except IndexError:
                              pass
                      elif name == "TI":
                          new_name = "title"
                          paper[new_name] = data
                      elif name == "AB":
                          new_name = "abstract"
                          paper[new_name] = data
                      elif name == "FAU" and authors_from_pubmed is None:
                          if "authors" not in paper:
                              paper["authors"] = []
                          surname, fns = data.split(",", 1)
                          fns = fns.strip()
                          fns = fns.split(" ") if " " in fns else [fns]
                          fns = " ".join(f"{n[0].upper()}." for n in fns)
                          author = f"{surname}, {fns}"
                          paper["authors"].append(author)
                      elif name == "JT":
                          data = re.sub(r"\([^)]*\)", "", data)
                          words = data.split()
                          ignore = ("on", "in", "of", "the", "and")
                          words = [w.title() if w not in ignore else w for w in words]
                          words = " ".join(words)
                          words = words.replace(".", ":")
                          paper["journal"] = words
                      elif "[doi]" in data and "doi" not in paper:
                          paper["doi"] = data.split()[0]
                      elif name == "PG":
                          paper["first_page"] = data
                          if "-" in data:
                              f, l = data.split("-")
                              if len(l) < len(f):
                                  d = len(f) - len(l)
                                  l = f[:d] + l
                              paper["first_page"], paper["last_page"] = (f, l)
                      elif name == "ED":
                          paper["editor"] = data
                      elif name == "CI":
                          paper["city"] = data
                      elif name == "CC":
                          paper["state"] = data
                      elif name == "CY":
                          paper["publisher"] = data
                      elif name == "CO":
                          paper["collection"] = data
                      elif name == "BN":
                          paper["book"] = data
                  else:
                      if name in ("TI", "AB"):
                          paper[new_name] += " " + l.strip()
      papers.append(paper)
  
      for paper in papers:
          authors = copy.copy(paper["authors"])
          if len(authors) > 1:
              authors[-1] = f"& {authors[-1]}"
          if len(authors) > 2:
              authors = ", ".join(authors)
          else:
              authors = " ".join(authors)
          authors = authors.replace("Mathias, S.", "<b>Mathias, S.</b>")
          authors = authors.replace("<b>Mathias, S.</b> R.", "<b>Mathias, S. R.</b>")
          paper["authors"] = authors
          if "pmid" in paper:
              if paper["pmid"] == "24389264":
                  paper["doi"] = "10.2741/S417"
          if "doi" in paper:
              paper["doi_link"] = "https://www.doi.org/" + paper["doi"]
          if "pmid" in paper:
              paper["pmid_link"] = "https://www.ncbi.nlm.nih.gov/pubmed/" + paper["pmid"]
          if "journal" in paper:
              if paper["journal"] == "Frontiers in Bioscience":
                  paper["journal"] = "Frontiers in Bioscience (Scholar Edition)"
              paper["journal"] = (
                  paper["journal"]
                  .replace(" : Cb", "")
                  .replace(" : the", "")
                  .replace(" Journal of the Association of European Psychiatrists", "")
                  .replace(" : Official Publication of the American College of", "")
                  .replace(" Official Journal of the Society For", "")
                  .replace("Jama", "JAMA")
              )
          paper["title"] = paper["title"][0] + paper["title"][1:].lower()
          paper["title"] = (
              paper["title"]
              .replace("african", "African")
              .replace("american", "American")
              .replace("qtl", "QTL")
              .replace("enigma", "ENIGMA")
              .replace("mri ", "MRI ")
          )
          for s in string.ascii_lowercase:
              paper["title"].replace(f": {s}", f": {s.upper()}")
          paper["authors"] = paper["authors"].replace(".., ", "")
  
      papers = [p for p in papers if int(p["year"]) >= 2010]
  
      with open("../../_data/my_papers.yaml", "w") as fw:
          fw.write("my_papers:\n")
          for paper in papers:
              if "Correction:" not in paper["title"]:
                  for k, v in paper.items():
                      v = v.replace('"', "'")
                      s = f"""{k}: "{v}"\n"""
                      if k == "id":
                          s = "\n - " + s
                      else:
                          s = "   " + s
                      fw.write(s)
  
  
  if __name__ == "__main__":
      main()
sdt-yn-experiment__py: |
  """Script to perform a simple at-home yes/no experiment and analyze the resulting data
  using signal detection theory.
  
  """
  import numpy as np
  from scipy.stats import norm
  import prettytable as pt
  import sounddevice as sd
  
  
  def trial(signal, n=None):
      """Performs a trial in the experiment.
  
      Args:
          signal (bool): Should the trial contain a tone?
          n (:obj:`bool`, optional): Trial number. If omitted, a "practice" trial is
              performed which will allow the observer an opportunity to change the volume
              settings on their computer.
  
      Returns:
          rsp (bool): On practice trials, this indicates whether the real experiment
              should begin. On real trials, it indicates whether the observer responded
              "yes".
  
      """
      t = np.arange(0, 0.1, 1 / 44100)
      tone = 1e-5 * 10 ** (50 / 20) * np.sin(2 * np.pi * 1000 * t + 0)
      noise = np.random.normal(size=len(t)) * tone.std() / np.sqrt(2)
      sd.play(noise + tone if signal and isinstance(n, int) else noise, 44100)
      responses = {"n": False, "y": True}
      if isinstance(n, int):
          instr = f"Trial {n}: Did you hear a tone? ([y] or [n])?"
      else:
          instr = "Adjust your volume settings until the noise barely audible."
          instr += "\n([y] to adjust and hear again; [n] to continue)"
      while 1:
          try:
              return responses[input(instr).lower()]
          except KeyError:
              pass
  
  
  def experiment():
      """Performs a series of trials.
  
      """
      adj = True
      while adj:
          adj = trial(False)
      X = [False, True] * 20
      np.random.shuffle(X)
      Y = [trial(*p[::-1]) for p in enumerate(X)]
      c = sum([1 for x, y in zip(X, Y) if x == 0 and y == 0])
      f = sum([1 for x, y in zip(X, Y) if x == 0 and y == 1])
      m = sum([1 for x, y in zip(X, Y) if x == 1 and y == 0])
      h = sum([1 for x, y in zip(X, Y) if x == 1 and y == 1])
      return c, f, m, h
  
  
  def sdt_yn(c, f, m, h):
      """Calcualte SDT statistics.
  
      """
      n = f + c
      s = m + h
      sens = norm.ppf(h / s) - norm.ppf(f / n)
      crit = -0.5 * (norm.ppf(h / s) + norm.ppf(f / n))
      return sens, crit
  
  
  if __name__ == "__main__":
  
      print(
          """
  ████████╗██╗  ██╗███████╗     ██████╗██████╗  █████╗  ██████╗██╗  ██╗███████╗██████╗ 
  ╚══██╔══╝██║  ██║██╔════╝    ██╔════╝██╔══██╗██╔══██╗██╔════╝██║ ██╔╝██╔════╝██╔══██╗
     ██║   ███████║█████╗      ██║     ██████╔╝███████║██║     █████╔╝ █████╗  ██║  ██║
     ██║   ██╔══██║██╔══╝      ██║     ██╔══██╗██╔══██║██║     ██╔═██╗ ██╔══╝  ██║  ██║
     ██║   ██║  ██║███████╗    ╚██████╗██║  ██║██║  ██║╚██████╗██║  ██╗███████╗██████╔╝
     ╚═╝   ╚═╝  ╚═╝╚══════╝     ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚═════╝ 
                                                                                       
  ██████╗  █████╗ ███████╗███████╗ ██████╗  ██████╗ ███╗   ██╗    ██╗██╗██╗██╗██╗██╗██╗
  ██╔══██╗██╔══██╗██╔════╝██╔════╝██╔═══██╗██╔═══██╗████╗  ██║    ██║██║██║██║██║██║██║
  ██████╔╝███████║███████╗███████╗██║   ██║██║   ██║██╔██╗ ██║    ██║██║██║██║██║██║██║
  ██╔══██╗██╔══██║╚════██║╚════██║██║   ██║██║   ██║██║╚██╗██║    ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
  ██████╔╝██║  ██║███████║███████║╚██████╔╝╚██████╔╝██║ ╚████║    ██╗██╗██╗██╗██╗██╗██╗
  ╚═════╝ ╚═╝  ╚═╝╚══════╝╚══════╝ ╚═════╝  ╚═════╝ ╚═╝  ╚═══╝    ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝                                                                              
  
  Welcome! This script performs a simple experiment and analyzes the data using signal
  detection theory (SDT)."""
      )
      c, f, m, h = experiment()
      print("Experiment done!")
      table = pt.PrettyTable()
      table.field_names = ["", "x = 0", "x = 1"]
      table.add_row(["y = 0", c, m])
      table.add_row(["y = 1", f, h])
      print("Here is your contingency table:")
      print(table)
      if any(x == 0 for x in (c, f, m, h)):
          print(
              """\
  Unfortunately, one or more of the cells has a value of 0. SDT statistics can't be
  calculated without applying some form of correction. Exiting now"""
          )
          exit()
      print("Calculating SDT statistics ...")
      sens, crit = sdt_yn(c, f, m, h)
      print("sensitivity (d') = %.2f" % sens)
      print("criterion (c) = %.2f" % crit)
ripple-sounds__py: |
  """Synthesize, plot, and play ripple sounds.
  
  """
  import numpy as np
  import sounddevice as sd
  import matplotlib.pyplot as plt
  
  from scipy.interpolate import interp1d
  from scipy.io import wavfile
  
  
  a0 = 1e-5  # reference amplitude
  sr = 44100  # sample rate
  
  
  def ripple_sound(dur, n, omega, w, delta, phi, f0, fm1, l=70):
      """Synthesizes a ripple sound.
  
      Args:
          dur (float): Duration of sound in s.
          n (int): Number of sinusoids.
          omega (:obj:`float` or `array`-like): Ripple density in Hz. Must be a single
              value or an array with length `duration * sr`.
          w (:obj:`float` or `array`-like): Ripple drift in Hz. Must be a single
              value or an array with length `duration * sr`.
          delta (:obj:`float` or `array`-like): Normalized ripple depth. Must be a single
              value or an array with length `duration * sr`. Value(s) must be in
              the range [0, 1].
          phi (float): Ripple starting phase in radians.
          f0 (float): Frequency of the lowest sinusoid in Hz.
          fm1 (float): Frequency of the highest sinusoid in Hz.
          l (:obj:`float`, optional): Level in dB of the sound, assuming a pure tone with
              peak amplitude `a0` is 0 dB SPL. TODO: Implement this correctly!
  
      Returns:
          y (np.array): The waveform.
          a (np.array): The envelope (useful for plotting).
  
      """
      # create sinusoids
      m = int(dur * sr)  # total number of samples
      shapea = (1, m)
      shapeb = (n, 1)
      t = np.linspace(0, dur, int(m)).reshape(shapea)
      i = np.arange(n).reshape(shapeb)
      f = f0 * (fm1 / f0) ** (i / (n - 1))
      sphi = 2 * np.pi * np.random.random(shapeb)
      s = np.sin(2 * np.pi * f * t + sphi)
  
      # create envelope
      x = np.log2(f / f0)
      if hasattr(w, "__iter__"):
          wprime = np.cumsum(w) / sr
      else:
          wprime = w * t
      a = 1 + delta * np.sin(2 * np.pi * (wprime + omega * x) + phi)
  
      # create the waveform
      y = (a * s / np.sqrt(f)).sum(axis=0)
  
      # scale to a given SPL
      # TODO: This is likely wrong; I haven't checked it
      y /= np.abs(y).max()
      y *= a0 * 10 ** (l / 20)
  
      return y, a
  
  
  def smooth_walk(points, dur):
      """Return a smooth walk.
  
      Args:
          points (:obj:`array`-like): Points to visit. These are spaced evenly and a
              spline is used to interpolate them.
          dur (float): Duration of sound in s.
  
      Returns:
          y (numpy.array): Values of the random walk.
  
      """
      f = interp1d(np.linspace(0, dur, len(points)), points, "cubic")
      return f(np.linspace(0, dur, dur * sr))
  
  
  def plot_env(a, ax, labels=False):
      """Plots an envelope onto an axis.
  
      Args:
          a (np.array): An array with shape (m, n) where m is the total number of samples
              and n in the number of sinusoids and values representing instantaneous
              amplitudes.
          ax (matplotlib.axes._subplots.AxesSubplot): Axis.
          labels (:obj:`bool`, optional): Include labels or not.
  
      """
      ax.pcolormesh(a, rasterized=True, vmin=0, vmax=2)
      ax.set_xticks([], [])
      ax.set_yticks([], [])
      if labels is True:
          ax.set_xlabel("Time")
          ax.set_ylabel("Frequency\n($log_2$ scale)")
  
  
  def main():
      """Create and plot some ripple sounds.
  
      """
      from matplotlib import rcParams
  
      figsize = rcParams["figure.figsize"]
      rcParams["figure.figsize"] = [figsize[0], int(figsize[1] / 2)]
      rcParams["font.size"] = 14
  
      # default parameter values
      np.random.seed(0)
      dur = 1
      n = 1000  # reduce this if you want to make figures; otherwise it takes forever!
      omega = 1
      w = 8
      delta = 0.9
      f0 = 250
      fm1 = 8000
      phi = 0.0
      args = (phi, f0, fm1)
  
      # filenames of figures
      fn = "../../assets/images/%s-ripples.svg"
  
      # static ripple sounds
      print("making static ripple sounds")
      _, axes = plt.subplots(1, 3, constrained_layout=True, sharex="all", sharey="all")
      for i, ax in enumerate(axes):
          _omega = 1.5 if i == 1 else omega
          _w = 0
          _delta = 0.5 if i == 2 else delta
          print(f"sound with omega={_omega:.2f}, w={_w:.2f}, and delta={_delta:.2f}")
          y, a = ripple_sound(dur, n, _omega, _w, _delta, *args)
          print("playing sound")
          sd.play(y, sr, blocking=True)
          print("plotting")
          plot_env(a, ax, ax == axes[0])
      print("saving a figure")
      plt.savefig(fn % "static", bbox_inches=0, transparent=True)
  
      # moving ripple sounds
      print("making moving ripple sounds")
      _, axes = plt.subplots(1, 3, constrained_layout=True, sharex="all", sharey="all")
      _ws = [4, 8, -4]
      for i, ax in enumerate(axes):
          _omega = omega
          _w = _ws[i]
          _delta = delta
          print(f"sound with omega={_omega:.2f}, w={_w:.2f}, and delta={_delta:.2f}")
          y, a = ripple_sound(dur, n, _omega, _w, _delta, *args)
          print("playing sound")
          sd.play(y, sr, blocking=True)
          print("plotting")
          plot_env(a, ax, ax == axes[0])
      print("making a figure")
      plt.savefig(fn % "moving", bbox_inches=0, transparent=True)
  
      # dynamic moving ripple sounds
      print("making dynamic static ripple sounds")
      _, axes = plt.subplots(1, 3, constrained_layout=True, sharex="all", sharey="all")
      for i, ax in enumerate(axes):
          _delta = smooth_walk(np.random.random(10), dur) if i == 0 else delta
          _omega = smooth_walk([1] * 5 + [1.5] * 5, dur) if i == 1 else omega
          _w = smooth_walk([-8, 0, 4, 8], dur) if i == 2 else w
          print(f"{[_delta, _omega, _w][i].shape}")
          y, a = ripple_sound(dur, n, _omega, _w, _delta, *args)
          print("playing sound")
          sd.play(y, sr, blocking=True)
          print("plotting")
          plot_env(a, ax, ax == axes[0])
      print("making a figure")
      plt.savefig(fn % "dynamic", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
realistic-funnel-a__py: |
  """Generate data from a more realistic hierarchical distribution.
  
  """
  import numpy as np
  import pymc3 as pm
  import matplotlib.pyplot as plt
  from matplotlib import rcParams
  from scipy.stats import norm, halfcauchy
  
  
  def main():
  
      # generate data
      np.random.seed(0)
      n = 1
      m = 10000
      mu = norm.rvs(0, 1, m)
      sigma = halfcauchy.rvs(0, 1, m)
      y = norm.rvs(mu, sigma, (n, m))
  
      # set up model
      with pm.Model():
  
          mu_ = pm.Normal("mu", 0, 1)
          sigma_ = pm.HalfCauchy("sigma", 1)
          y_ = pm.Normal("y", mu_, sigma_, shape=n)
  
          # sample and save samples
          trace = pm.sample(m, chains=1)
          mu_samples = trace["mu"][:]
          sigma_samples = trace["sigma"][:]
          y_samples = trace["y"].T[:]
  
      # plot 2-D figures
      sc = 5
      fs = rcParams["figure.figsize"]
      rcParams["figure.figsize"] = (fs[0], fs[0])
      rcParams["lines.linewidth"] = 2
      rcParams["font.size"] = 14
      fig, axes = plt.subplots(2, 2, constrained_layout=True, sharex=True)
  
      ax = axes[0, 0]
      ax.scatter(y[0], mu, marker=".", alpha=0.05, rasterized=True)
      ax.set_xlim(-sc, sc)
      ax.set_ylim(-sc, sc)
      ax.set_ylabel("$\mu$")
  
      ax = axes[0, 1]
      ax.scatter(
          y_samples[0], mu_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_ylim(-sc, sc)
      ax.set_yticklabels([])
  
      ax = axes[1, 0]
      ax.scatter(y[0], sigma, marker=".", alpha=0.05, rasterized=True)
      ax.set_ylim(0, sc / 2)
      ax.set_xlabel("$y_0$")
      ax.set_ylabel("$\sigma$")
      ax = axes[1, 1]
      ax.scatter(
          y_samples[0], sigma_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_ylim(0, sc / 2)
      ax.set_yticklabels([])
      ax.set_xlabel("$y_0$")
  
      # save
      plt.savefig("../images/realistic-funnel-a.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
pure-tones__py: |
  """Contains a function to generate pure tones.
  
  """
  import numpy as np
  import sounddevice as sd
  
  a0 = 1e-5  # reference amplitude
  sr = 44100  # sample rate
  
  
  def sinusoid(d, f, phi, l, a0=a0, sr=sr):
      """Generates a pure tone.
  
      A pure tone or sinusoid is a periodic waveform that is some variation on the sine
      wave.
  
      Args:
          d (float): Duration in s.
          f (float): Ordinary in Hz.
          phi (float): Starting phase in rad.
          l (float): Level in dB.
          a0 (:obj:`float`, optional): Amplitude of a 0-dB tone. Default is 1e-5.
          sr (:obj:`int`, optional): Sample rate in Hz. Default is 44100.
  
      Returns:
          waveform (np.ndarray): Sinusoidal waveform.
  
      """
      t = np.arange(0, int(round(d * sr))) / sr
      return a0 * 10 ** (l / 20) * np.sin(2 * np.pi * f * t + phi)
  
  
  if __name__ == "__main__":
  
      tone = sinusoid(1, 1000, 0, 60)
      sd.play(tone, 44100)
      sd.wait()
neals-funnel-c__py: |
  """Generate data and sample from Neal's funnel distribution.
  
  """
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib import rcParams
  import pymc3 as pm
  from scipy.stats import norm
  
  
  def main():
  
      with pm.Model():
  
          # set up figure
          fs = rcParams["figure.figsize"]
          rcParams["figure.figsize"] = (fs[0], fs[0] / 2)
          rcParams["lines.linewidth"] = 2
          rcParams["font.size"] = 14
  
          # simulate data
          np.random.seed(0)
          k = 9
          n = 10000
          v = norm.rvs(0, 3, n)
          x = norm.rvs(0, np.exp(v / 2), (k, n))
  
          # set up model
          v_ = pm.Normal("v", mu=0, sd=3)
          xt_ = pm.Normal("xt", mu=0, sd=1, shape=k)
          x_ = pm.Deterministic("x", pm.math.exp(v_ / 2) * xt_)
  
          # sample and save samples
          trace = pm.sample(n, chains=1)
          v_samples = trace["v"][:]
          xt_samples = trace["xt"][:].T
          x_samples = trace["x"][:].T
  
          # plot samples
          # plot simulated data
          fig, axes = plt.subplots(1, 2, constrained_layout=True)
          ax = axes[0]
          ax.scatter(
              xt_samples[0], v_samples, marker=".", alpha=0.05, rasterized=True, color="r"
          )
          ax.set_xlim(-3.5, 3.5)
          ax.set_ylim(-9, 9)
          ax.set_xlabel(r"$\tilde{x}_0$")
          ax.set_ylabel("$v$")
          ax = axes[1]
          ax.scatter(
              x_samples[0], v_samples, marker=".", alpha=0.05, rasterized=True, color="r"
          )
          ax.set_xlabel("$x_0$")
          ax.set_xlim(-20, 20)
          ax.set_ylim(-9, 9)
  
          # save
          plt.savefig("../images/neals-funnel-c.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
realistic-funnel-b__py: |
  """Generate data from a more realistic hierarchical distribution.
  
  """
  import numpy as np
  import pymc3 as pm
  import matplotlib.pyplot as plt
  from matplotlib import rcParams
  from scipy.stats import norm, halfcauchy
  
  
  def main():
  
      # generate data
      np.random.seed(0)
      n = 1
      m = 10000
      mu = norm.rvs(0, 1, m)
      sigma = halfcauchy.rvs(0, 1, m)
      y = norm.rvs(mu, sigma, (n, m))
  
      # set up model
      with pm.Model():
  
          mu_ = pm.Normal("mu", 0, 1)
          sigma_ = pm.HalfCauchy("sigma", 1)
          yt_ = pm.Normal("yt", 0, 1, shape=n)
          pm.Deterministic("y", mu_ + yt_ * sigma_)
          # y_ = pm.Normal("y", mu_, sigma_, shape=n)
  
          # sample and save samples
          trace = pm.sample(m, chains=1)
          mu_samples = trace["mu"][:]
          sigma_samples = trace["sigma"][:]
          yt_samples = trace["yt"].T[:]
          y_samples = trace["y"].T[:]
  
      # plot 2-D figures
      sc = 5
      fs = rcParams["figure.figsize"]
      rcParams["figure.figsize"] = (fs[0], fs[0])
      rcParams["lines.linewidth"] = 2
      rcParams["font.size"] = 14
      fig, axes = plt.subplots(2, 2, constrained_layout=True)
  
      ax = axes[0, 0]
      ax.scatter(
          yt_samples[0], mu_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_xlim(-sc, sc)
      ax.set_ylim(-sc, sc)
      ax.set_ylabel("$\mu$")
      ax.set_xticklabels([])
  
      ax = axes[0, 1]
      ax.scatter(
          y_samples[0], mu_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_xlim(-sc, sc)
      ax.set_ylim(-sc, sc)
      ax.set_yticklabels([])
      ax.set_xticklabels([])
  
      ax = axes[1, 0]
      ax.scatter(
          yt_samples[0], sigma_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_xlim(-sc, sc)
      ax.set_ylim(0, sc / 2)
      ax.set_xlabel(r"$\tilde{y}_0$")
      ax.set_ylabel("$\sigma$")
  
      ax = axes[1, 1]
      ax.scatter(
          y_samples[0], sigma_samples, marker=".", alpha=0.05, rasterized=True, color="r"
      )
      ax.set_xlim(-sc, sc)
      ax.set_ylim(0, sc / 2)
      ax.set_yticklabels([])
      ax.set_xlabel("$y_0$")
  
      # save
      plt.savefig("../images/realistic-funnel-b.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
refs2latex__py: |
  import yaml
  
  
  def main():
  
      lines = ""
      with open("../../_data/my_papers.yaml") as f:
  
          data = yaml.load(f, Loader=yaml.FullLoader)
          data = sorted(data["my_papers"], key=lambda i: i["sort"])[::-1]
  
          for paper in data:
  
              line = "%"
              authors = paper["authors"].replace("</b>", "}").replace("<b>", r"\textbf{")
              authors = r"\item " + authors.replace("&", r"\&")
              line += authors
              line += f" ({paper['year']}). {paper['title']} "
              line += paper["editor"] if "editor" in paper else ""
  
              # this is a book
              if "book" in paper:
                  line += r" \emph{" + paper["book"] + "}"
                  if "collection" in paper and "volume" in paper:
                      line += f" ({paper['collection']} vol.~{paper['volume']}, pp.~{paper['first_page']}--{paper['last_page']})."
                  elif "book" in paper and "volume" in paper:
                      line += f" (vol.~{paper['volume']}, pp.~{paper['first_page']}--{paper['last_page']})."
                  elif "book" in paper and "first_page" in paper:
                      line += f" (pp.~{paper['first_page']}--{paper['last_page']})."
                  else:
                      line += "."
                  line += f" {paper['city']}, {paper['state']}: {paper['publisher']}."
  
              # this is an article
              else:
                  line += r"\emph{" + paper["journal"] + "}"
                  if "volume" in paper:
                      line += ", \emph{" + paper["volume"] + "}"
                  if "issue" in paper:
                      line += f"({paper['issue']})"
                  if "first_page" in paper:
                      line += f", {paper['first_page']}"
                  if "last_page" in paper:
                      line += f"--{paper['last_page']}."
                  else:
                      line += "."
  
              # hyperlinks
              if "doi" in paper:
                  line += (
                      " DOI:~\href{"
                      + f"https://doi.org/{paper['doi']}"
                      + "}{"
                      + paper["doi"]
                      + "}."
                  )
              if "pmid" in paper:
                  line += (
                      " PMID:~\href{"
                      + f"https://www.ncbi.nlm.nih.gov/pubmed/?term={paper['pmid']}"
                      + "}{"
                      + paper["pmid"]
                      + "}."
                  )
  
              """
              {% if paper.journal %}
      {% if paper.volume or paper.first_page %}
        <i>{{ paper.journal }}</i>,
        {% if paper.volume %}
          {% if paper.issue %}
            {% if paper.first_page %}
              <i>{{ paper.volume }}</i>({{ paper.issue }}),
            {% else %}
              <i>{{ paper.volume }}</i>({{ paper.issue }}).
            {% endif %}
          {% else %}
            {% if paper.first_page %}
              <i>{{ paper.volume }}</i>,
            {% else %}
              <i>{{ paper.volume }}</i>.
            {% endif %}
          {% endif %}
        {% endif %}
        {% if paper.first_page %}
          {% if paper.last_page %}
            {{ paper.first_page }}–{{paper.last_page }}.   
          {% else %}
            {{ paper.first_page }}.
          {% endif %}
        {% endif %}
      {% else %}
        <i>{{ paper.journal }}</i>.
      {% endif %}
    {% endif %}
    {% if paper.doi %}
      DOI: <a href="https://doi.org/{{ paper.doi }}">{{ paper.doi }}</a>.
    {% endif %}
    {% if paper.pmid %}
      PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/?term={{ paper.pmid }}">{{ paper.pmid }}</a>.
    {% endif %}
              
              """
  
              print(line)
  
  
  if __name__ == "__main__":
  
      main()
play-tone-interactively__py: |
  """Simply generates a pure tone using numpy and plays it via sounddevice.
  
  As always, make sure your volume settings are low before running this script, especially
  if you are using headphones!
  
  """
  import numpy as np
  import sounddevice as sd
  
  
  if __name__ == "__main__":
  
      tone = np.sin(2 * np.pi * 440 * np.arange(0, 1, 1 / 44100))  # generate the tone
      sd.play(tone, 44100)  # play it
      sd.wait()  # wait for the tone to finish
equal-loudness__py: |
  import numpy as np
  from scipy import interpolate
  import matplotlib.pyplot as plt
  from matplotlib import rcParams as defaults
  
  f = np.array(
      [
          20,
          25,
          31.5,
          40,
          50,
          63,
          80,
          100,
          125,
          160,
          200,
          250,
          315,
          400,
          500,
          630,
          800,
          1000,
          1250,
          1600,
          2000,
          2500,
          3150,
          4000,
          5000,
          6300,
          8000,
          10000,
          12500,
      ]
  )
  af = np.array(
      [
          0.532,
          0.506,
          0.480,
          0.455,
          0.432,
          0.409,
          0.387,
          0.367,
          0.349,
          0.330,
          0.315,
          0.301,
          0.288,
          0.276,
          0.267,
          0.259,
          0.253,
          0.250,
          0.246,
          0.244,
          0.243,
          0.243,
          0.243,
          0.242,
          0.242,
          0.245,
          0.254,
          0.271,
          0.301,
      ]
  )
  Lu = np.array(
      [
          -31.6,
          -27.2,
          -23.0,
          -19.1,
          -15.9,
          -13.0,
          -10.3,
          -8.1,
          -6.2,
          -4.5,
          -3.1,
          -2.0,
          -1.1,
          -0.4,
          0.0,
          0.3,
          0.5,
          0.0,
          -2.7,
          -4.1,
          -1.0,
          1.7,
          2.5,
          1.2,
          -2.1,
          -7.1,
          -11.2,
          -10.7,
          -3.1,
      ]
  )
  Tf = np.array(
      [
          78.5,
          68.7,
          59.5,
          51.1,
          44.0,
          37.5,
          31.5,
          26.5,
          22.1,
          17.9,
          14.4,
          11.4,
          8.6,
          6.2,
          4.4,
          3.0,
          2.2,
          2.4,
          3.5,
          1.7,
          -1.3,
          -4.2,
          -6.0,
          -5.4,
          -1.5,
          6.0,
          12.6,
          13.9,
          12.3,
      ]
  )
  
  
  def elc(phon, frequencies=None):
      """Returns an equal-loudness contour.
  
      Args:
          phon (float): Phon value of the contour.
          frequencies (:obj:`np.ndarray`, optional): Frequencies to evaluate. If not
              passed, all 29 points of the ISO standard are returned. Any frequencies not
              present in the standard are found via spline interpolation.
  
      Returns:
          contour (np.ndarray): db SPL values.
  
      """
      assert 0 <= phon <= 90, f"{phon} is not [0, 90]"
      Ln = phon
      Af = (
          4.47e-3 * (10 ** (0.025 * Ln) - 1.15)
          + (0.4 * 10 ** (((Tf + Lu) / 10) - 9)) ** af
      )
      Lp = ((10.0 / af) * np.log10(Af)) - Lu + 94
  
      if frequencies is not None:
  
          assert frequencies.min() >= f.min(), "Frequencies are too low"
          assert frequencies.max() <= f.max(), "Frequencies are too high"
          tck = interpolate.splrep(f, Lp, s=0)
          Lp = interpolate.splev(frequencies, tck, der=0)
  
      return Lp
  
  
  def plot_elcs():
      """Makes the equal-loudness-contour plot.
  
      """
      defaults["lines.linewidth"] = 2
      defaults["font.size"] = 14
  
      fig, ax = plt.subplots(1, 1, constrained_layout=True)
      x = np.logspace(np.log10(f.min()), np.log10(f.max()), 1000)
  
      for p in range(0, 100, 10):
          c, l = ("C0", None) if p != 60 else ("C1", "60 phon")
          ax.plot(x, elc(p, x), c=c, label=l)
  
      ax.legend(fancybox=False, framealpha=0)
      ax.set_xscale("log")
      ax.set_xlabel("Frequency (Hz)")
      ax.set_ylabel("Sound pressure level (dB)")
      plt.savefig(
          f"../../assets/images/equal-loudness-contours.svg",
          bbox_inches=0,
          transparent=True,
      )
  
  
  if __name__ == "__main__":
  
      plot_elcs()
neals-funnel-a__py: |
  """Generate data from Neal's funnel distribution.
  
  """
  import numpy as np
  import matplotlib.pyplot as plt
  from matplotlib import rcParams
  from scipy.stats import norm
  
  
  def main():
  
      # set up figure
      fs = rcParams["figure.figsize"]
      rcParams["figure.figsize"] = (fs[0], fs[0] / 2)
      rcParams["lines.linewidth"] = 2
      rcParams["font.size"] = 14
  
      # generate data
      np.random.seed(0)
      k = 9
      n = 10000
      v = norm.rvs(0, 3, n)
      x = norm.rvs(0, np.exp(v / 2), (k, n))
  
      # plot data
      fig, axes = plt.subplots(1, 2, constrained_layout=True)
      ax = axes[0]
      ax.scatter(x[0], v, marker=".", alpha=0.05, rasterized=True)
      ax.set_xlim(-20, 20)
      ax.set_ylim(-9, 9)
      ax.set_xlabel("$x_0$")
      ax.set_ylabel("$v$")
  
      # plot analytic log-likelihood
      ax = axes[1]
      r = 500
      x, v = np.meshgrid(np.linspace(-20, 20, r), np.linspace(-9, 9, r))
      logp = norm.logpdf(v, 0, 3) + norm.logpdf(x, 0, np.exp(v / 2))
      ax.imshow(logp, vmin=-7.5, vmax=-2.5, cmap="viridis", origin="lower")
      ax.set_yticks([])
      ax.set_yticklabels([])
      ax.set_xticks(np.linspace(0, 499, 5))
      ax.set_xticklabels(np.linspace(-20, 20, 5).astype(int))
      ax.set_xlabel("$x_0$")
  
      # save
      plt.savefig("../images/neals-funnel-a.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
      main()
make_my_bib__py: |
  """Create the bibliography file my_papers.yml.
  
  """
  
  from Bio import Entrez
  import re
  
  
  def make_my_bib():
      """Grab all my publications and format them into a YAML bibliography.
  
      """
      Entrez.email = "your.email@example.com"
      handle = Entrez.esearch(
          db="pubmed", sort="date", retmax="200", retmode="xml", term="mathias sr[author]"
      )
      pmids = Entrez.read(handle)["IdList"]
      extras = ["28480992", "28385874", "27744290"]
      pmids += extras
      pmids = set(pmids)
      pmids = ",".join(pmids)
      handle = Entrez.efetch(db="pubmed", retmode="xml", id=pmids)
      papers = Entrez.read(handle)["PubmedArticle"]
      data = []
  
      for paper in papers:
  
          article = paper["MedlineCitation"]["Article"]
          journal = article["Journal"]
          date = journal["JournalIssue"]["PubDate"]
          year = date["Year"]
          month = "00" if "Month" not in date else date["Month"]
          if len(month) == 3:
              month = dict(
                  zip(
                      [
                          "Jan",
                          "Feb",
                          "Mar",
                          "Apr",
                          "May",
                          "Jun",
                          "Jul",
                          "Aug",
                          "Sep",
                          "Oct",
                          "Nov",
                          "Dec",
                      ],
                      range(1, 13),
                  )
              )[month]
          sort = year + "%02d" % int(month)
          ids = paper["PubmedData"]["ArticleIdList"]
  
          authors = []
          for _a in article["AuthorList"]:
              if "LastName" in _a:
                  a = _a["LastName"] + ", " + ". ".join(_a["Initials"]) + "."
              else:
                  a = _a["CollectiveName"].rstrip()
              authors.append(a)
          if len(authors) > 1:
              authors[-1] = "& " + authors[-1]
  
          jt = journal["Title"].title().replace("Of The", "of the").replace("And", "and")
          jt = jt.replace("Of", "of").replace(" (New York, N.Y. : 1991)", "")
          jt = jt.split(" : ")[0]
          jt = jt.replace(". ", ": ").replace("In ", "in ").replace(": Cb", "")
          jt = jt.replace("Jama", "JAMA")
  
          ji = journal["JournalIssue"]
          volume = None if "Volume" not in ji else ji["Volume"]
          issue = None if "Issue" not in ji else ji["Issue"]
  
          title = article["ArticleTitle"]
          rtn = re.split("([:] *)", title)
          title = "".join([i.capitalize() for i in rtn])
          keep = ["BP1-BP2", "African", "American", "Americans", "MRI", "QTL", "ENIGMA"]
          title = " ".join(s.upper() if s.upper() in keep else s for s in title.split())
  
          k = "Pagination"
          first_page = None if k not in article else article[k]["MedlinePgn"]
          last_page = None
          if first_page:
              first_page, *last_page = first_page.split("-")
  
          dic = {
              "authors": ", ".join(authors),
              "title": title if title[-1] != "." else title[:-1],
              "journal": jt,
              "year": year,
              "sort": sort,
              "pmid": str(paper["MedlineCitation"]["PMID"]),
              "doi": [str(i) for i in ids if str(i)[:3] == "10."][0].lower(),
          }
          dic["id"] = dic["pmid"]
  
          if volume:
              dic["volume"] = volume
          if issue:
              dic["issue"] = issue
          if first_page:
              dic["first_page"] = first_page
          if last_page:
              dic["last_page"] = last_page[0]
  
          if int(year) >= 2010 and "Correction:" not in title:
  
              data.append(dic)
  
      with open("../../_data/my_papers.yaml", "w") as fw:
  
          fw.write("my_papers:\n")
  
          for paper in data:
  
              fw.write(f"""\n - id: "{paper['id']}"\n""")
              del paper["id"]
              [fw.write(f"""   {k}: "{v}"\n""") for k, v in paper.items()]
  
          s = "".join(open("../../_data/my_papers_manual.yaml").readlines()[1:])
          fw.write(s)
  
  
  if __name__ == "__main__":
      make_my_bib()
ddm-fig__py: |
  """Create a DDM figure.
  
  """
  import hddm
  import numpy as np
  from scipy.stats import norm
  import matplotlib.pyplot as plt
  from matplotlib.gridspec import GridSpec
  from tqdm import tqdm
  
  
  def setupfig():
      """Tweak for the target journal.
  
      """
      fig = plt.figure()
      gs = GridSpec(3, 1, height_ratios=[1, 2, 1], hspace=0)
      return fig, gs
  
  
  def delabel(ax):
      """Strip labels.
  
      """
      ax.xaxis.set_ticklabels([])
      ax.yaxis.set_ticklabels([])
      ax.xaxis.set_ticks([])
      ax.yaxis.set_ticks([])
  
  
  def kde(ax, x, mx, c):
      """Plot a KDE for reaction times.
  
      """
      x = x[x <= mx]
      bandwidth = 0.8 * x.std() * x.size ** (-1 / 5.0)
      support = np.linspace(0, mx, 500)
      kernels = []
  
      for x_i in tqdm(x):
  
          kernel = norm(x_i, bandwidth).pdf(support)
          kernels.append(kernel)
          density = np.sum(kernels, axis=0)
  
      my = np.max(density)
      ax.plot(support, density, c=c)
      ax.fill_between(support, 0, density, alpha=0.5, facecolor=c)
      ax.set_ylim(0, my * 1.05)
      delabel(ax)
  
      return my
  
  
  def traces(ax, n, mx, **params):
      """Draw example diffusions.
  
      """
      x = np.linspace(0, mx, 101)
      delta = x[1]
      nd_samples = np.round(params["t"] / delta).astype(int)
      d_samples = len(x) - nd_samples
      y0 = np.zeros(nd_samples) * np.nan
      y0[-1] = 0
  
      for i in range(n):
  
          y1 = np.cumsum(norm.rvs(params["v"] * delta, np.sqrt(delta), size=d_samples))
          y = params["a"] * params["z"] + np.concatenate([y0, y1])
  
          try:
  
              idx1 = np.where(y > params["a"])[0][0] + 1
  
          except:
  
              idx1 = np.inf
  
          try:
  
              idx2 = np.where(y < 0)[0][0] + 1
  
          except:
  
              idx2 = np.inf
  
          if idx1 < idx2:
  
              y[idx1:] = np.nan
              ax.plot(x, y, c="C0", zorder=-12, alpha=0.25)
  
          if idx2 < idx1:
  
              y[idx2:] = np.nan
              ax.plot(x, y, c="C3", zorder=-11, alpha=0.25)
  
          ax.set_ylim(0, params["a"])
          ax.set_xlim(0, mx)
          delabel(ax)
  
  
  def ddmfig(**params):
      """Draw a DDM plot with the given parameter values.
  
      """
      mx = 3.5  # max x-value; adjust this if simulated RTs are slower/faster
      size = 1500  # increase this number for better KDEs
      ntraces = 2  # increase this for more example diffusions
  
      # set up fig
      fig, gs = setupfig()
  
      # traces
      ax = plt.subplot(gs[1])
      traces(ax, ntraces, mx, **params)
  
      # data for kdes
      df, _ = hddm.generate.gen_rand_data(params, subjs=1, size=size)
  
      # top KDE
      ax = plt.subplot(gs[0])
      my = kde(ax, df[df.response == 1].rt.values, mx, "C0")
  
      # bottom KDE
      ax = plt.subplot(gs[2])
      kde(ax, df[df.response == 0].rt.values, mx, "C3")
      ax.set_ylim(0, my * 1.05)
      ax.invert_yaxis()
  
      # remove whitespace around fig
      plt.tight_layout(0)
  
  
  def main():
  
      np.random.seed(25)
      ddmfig(v=0.7, a=1.5, t=0.6, z=0.5)
      plt.savefig("assets/scripts/ddm-fig.svg", bbox_inches=0, transparent=True)
  
  
  if __name__ == "__main__":
  
      main()
sdt-figs__py: |
  """Create SDT figures.
  
  """
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sb
  from scipy.stats import norm
  
  if __name__ == "__main__":
  
      from matplotlib import rcParams as defaults
  
      figsize = defaults["figure.figsize"]
      defaults["figure.figsize"] = [figsize[0], int(figsize[0] / 2)]
      defaults["lines.linewidth"] = 2
      defaults["font.size"] = 14
  
      r = 500
      d = 2
      q = 4
      psi = np.linspace(-q, q + d, r)
      fig, ax = plt.subplots(1, 1, constrained_layout=True)
      ax.plot(psi, norm.pdf(psi), label=r"$X=0$")
      ax.plot(psi, norm.pdf(psi, d, 1), label=r"$X=1$")
      ax.set_xticks([], [])
      ax.set_yticks([], [])
      sb.despine(fig, ax, top=True, right=True)
      ax.set_xlabel("$\Psi$")
      ax.set_xticks([0, d])
      ax.set_xticklabels([0, "$d$"])
      ax.set_ylabel("Probability density")
      ax.set_xlim(psi.min(), psi.max())
      ax.set_ylim(0, norm.pdf(psi).max() * 1.1)
      ax.legend(fancybox=False, framealpha=0)
      plt.savefig(
          f"../../assets/images/sdt-evg-perceptual.svg", bbox_inches=0, transparent=True
      )
  
      k = 0.8
      ax.set_xticks([0, k, d])
      ax.set_xticklabels([0, "$k$", "$d$"])
      ax.vlines([k], 0, norm.pdf(psi).max(), zorder=10)
      plt.savefig(
          f"../../assets/images/sdt-evg-response.svg", bbox_inches=0, transparent=True
      )
  
      ix = np.argmin(np.abs(psi - k))
      x = psi[ix]
      fill = ax.fill_between(psi[ix:], norm.pdf(psi[ix:], 0, 1), alpha=0.3)
      plt.savefig(f"../../assets/images/sdt-evg-fa.svg", bbox_inches=0, transparent=True)
      fill.remove()
  
      fill = ax.fill_between(psi[ix:], norm.pdf(psi[ix:], d, 1), alpha=0.3)
      plt.savefig(f"../../assets/images/sdt-evg-h.svg", bbox_inches=0, transparent=True)
      fill.remove()
code2yaml__py: |
  """code2yaml.py
  
  Convert contents of `scripts` directory to YAML.
  
  """
  import os
  import black
  
  
  def code2yaml():
      """Convert contents of `scripts` directory to YAML.
  
      """
  
      for i, f in enumerate(os.listdir(os.getcwd())):
  
          src = os.path.join(os.getcwd(), f)
          try:
              contents = open(src).read()
              kwargs = {"fast": True, "mode": black.FileMode()}
              _, ext = os.path.splitext(f)
              if ext == ".py":
                  try:
                      # for some reason I can't get "in-place" formatter to work
                      contents = black.format_file_contents(contents, **kwargs)
                  except black.NothingChanged:
                      pass
              open(src, "w").write(contents)
              contents = "  " + contents.replace("\n", "\n  ").rstrip() + "\n"
              contents = f"{f.replace('.', '__')}: |\n" + contents
              open("../../_data/code.yaml", "w" if i == 0 else "a").write(contents)
          except IsADirectoryError:
              pass
  
  
  if __name__ == "__main__":
      code2yaml()
play_tone__py: |
  """Simply generates a pure tone using numpy and plays it via sounddevice.
  
  As always, make sure your volume settings are low before running this script, especially
  if you are using headphones!
  
  """
  import numpy as np
  import sounddevice as sd
  
  if __name__ == "__main__":
      tone = np.sin(2 * np.pi * 440 * np.arange(0, 1, 1 / 44100))  # generate the tone
      sd.play(tone, 44100)  # play it
      sd.wait()  # wait for the tone to finish
bcfa_long__py: |
  """Example of Bayesian confirmatory factor analysis in PyMC3.
  
  """
  import numpy as np
  import pandas as pd
  import pymc3 as pm
  import theano.tensor as tt
  import matplotlib.pyplot as plt
  from os.path import exists
  
  from matplotlib import rcParams
  from pymc3.math import matrix_dot
  from tabulate import tabulate
  
  
  def bcfa(
      items,
      factors,
      beta="estimate",
      nu_sd=2.5,
      alpha_sd=2.5,
      d_beta=2.5,
      corr_items=True,
      corr_factors=True,
      g_eta=100,
      l_eta=1,
      beta_beta=1,
  ):
      r"""Constructs a Bayesian CFA model.
  
      Args:
          items (np.array): Array of item data.
          factors (np.array): Factor design matrix.
          beta (:obj:`float` or `'estimate'`, optional): Standard deviation of normal
              prior on cross loadings. If `'estimate'`,  beta is estimated from the data.
          nu_sd (:obj:`float`, optional): Standard deviation of normal prior on item
              intercepts.
          alpha_sd (:obj:`float`, optional): Standard deviation of normal prior on factor
              intercepts.
          d_beta (:obj:`float`, optional): Scale parameter of half-Cauchy prior on factor
              standard deviation.
          corr_factors (:obj:`bool`, optional): Allow correlated factors.
          corr_items (:obj:`bool`, optional): Allow correlated items.
          g_eta (:obj:`float`, optional): Shape parameter of LKJ prior on residual item
              correlation matrix.
          l_eta (:obj:`float`, optional): Shape parameter of LKJ prior on factor
              correlation matrix.
          beta_beta (:obj:`float`, optional): Beta parameter of beta prior on beta.
  
      Returns:
  
          None: Places model in context.
  
      """
      # get numbers of cases, items, and factors
      n, p = items.shape
      p_, m = factors.shape
      assert p == p_, "Mismatch between data and factor-loading matrices"
  
      # place priors on item and factor intercepts
      nu = pm.Normal(name=r"$\nu$", mu=0, sd=nu_sd, shape=p, testval=items.mean(axis=0))
      alpha = pm.Normal(name=r"$\alpha$", mu=0, sd=alpha_sd, shape=m, testval=np.zeros(m))
  
      # place priors on unscaled factor loadings
      Phi = pm.Normal(name=r"$\Phi$", mu=0, sd=1, shape=factors.shape, testval=factors)
  
      # create masking matrix for factor loadings
      if isinstance(beta, str):
          assert beta == "estimate", f"Don't know what to do with '{beta}'"
          beta = pm.Beta(name=r"$\beta$", alpha=1, beta=beta_beta, testval=0.1)
      M = (1 - np.asarray(factors)) * beta + np.asarray(factors)
  
      # create scaled factor loadings
      Lambda = pm.Deterministic(r"$\Lambda$", Phi * M)
  
      # determine item means
      mu = nu + matrix_dot(Lambda, alpha)
  
      # place priors on item standard deviations
      D = pm.HalfCauchy(name=r"$D$", beta=d_beta, shape=p, testval=items.std(axis=0))
  
      # place priors on item correlations
      f = pm.Lognormal.dist(sd=0.25)
      if not corr_items:
          Omega = np.eye(p)
      else:
          G = pm.LKJCholeskyCov(name=r"$G$", eta=g_eta, n=p, sd_dist=f)
          ch1 = pm.expand_packed_triangular(p, G, lower=True)
          K = tt.dot(ch1, ch1.T)
          sd1 = tt.sqrt(tt.diag(K))
          Omega = pm.Deterministic(r"$\Omega$", K / sd1[:, None] / sd1[None, :])
  
      # determine residual item variances and covariances
      Theta = pm.Deterministic(r"$\Theta$", D[None, :] * Omega * D[:, None])
  
      # place priors on factor correlations
      if not corr_factors:
          Psi = np.eye(m)
      else:
          L = pm.LKJCholeskyCov(name=r"$L$", eta=l_eta, n=m, sd_dist=f)
          ch = pm.expand_packed_triangular(m, L, lower=True)
          Gamma = tt.dot(ch, ch.T)
          sd = tt.sqrt(tt.diag(Gamma))
          Psi = pm.Deterministic(r"$\Psi$", Gamma / sd[:, None] / sd[None, :])
  
      # determine variances and covariances of items
      Sigma = matrix_dot(Lambda, Psi, Lambda.T) + Theta
  
      # place priors on observations
      pm.MvNormal(name="$Y$", mu=mu, cov=Sigma, observed=items, shape=items.shape)
  
  
  def main():
  
      # load the data
      df = pd.read_csv("../../assets/data/HS.csv", index_col=0)
  
      # define items to keep
      item_names = [
          "visual",
          "cubes",
          "paper",
          "flags",
          "general",
          "paragrap",
          "sentence",
          "wordc",
          "wordm",
          "addition",
          "code",
          "counting",
          "straight",
          "wordr",
          "numberr",
          "figurer",
          "object",
          "numberf",
          "figurew",
      ]
  
      # define the factor structure
      factors = np.array(
          [
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [1, 0, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 1, 0, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 1, 0],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
              [0, 0, 0, 1],
          ]
      )
  
      # iterate over the two schools
      for school, sdf in df.groupby("school"):
  
          # define the path to save results
          f = f"../data/{school}"
  
          # select the 19 commonly used variables
          items = sdf[item_names]
  
          # for numerical convenience, standardize the data
          items = (items - items.mean()) / items.std()
  
          with pm.Model():
  
              # construct the model
              bcfa(items, factors)
  
              if not exists(f):
  
                  # sample and save
                  trace = pm.sample(19000, tune=1000, chains=2)
                  pm.save_trace(trace, f)
                  pm.traceplot(trace, compact=True)
                  rcParams["font.size"] = 14
                  plt.savefig(f"{f}/traceplot.svg")
  
              else:
  
                  trace = pm.load_trace(f)
  
          # create a nice summary table
          loadings = pd.DataFrame(
              trace[r"$\Lambda$"].mean(axis=0).round(3),
              index=[v.title() for v in item_names],
              columns=["Spatial", "Verbal", "Speed", "Memory"],
          )
          loadings.to_csv(f"{f}/loadings.csv")
          print(tabulate(loadings, tablefmt="pipe", headers="keys"))
          correlations = pd.DataFrame(
              trace[r"$\Psi$"].mean(axis=0).round(3),
              index=["Spatial", "Verbal", "Speed", "Memory"],
              columns=["Spatial", "Verbal", "Speed", "Memory"],
          )
          correlations.to_csv(f"{f}/factor_correlations.csv")
          print("\n")
          print(tabulate(correlations, tablefmt="pipe", headers="keys"))
          correlations = pd.DataFrame(
              trace[r"$\Omega$"].mean(axis=0).round(3),
              index=item_names,
              columns=item_names,
          )
          correlations.to_csv(f"{f}/item_correlations.csv")
  
  
  if __name__ == "__main__":
      main()
noise-tone-plus-noise__py: |
  """Create figurea to illustrate noise and noise plus a pure tone.
  
  """
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sb
  
  
  if __name__ == "__main__":
  
      from matplotlib import rcParams as defaults
  
      figsize = defaults["figure.figsize"]
      defaults["figure.figsize"] = [figsize[0], int(figsize[1] / 2)]
      defaults["lines.linewidth"] = 2
      defaults["font.size"] = 14
  
      n = 2000
      x = np.linspace(0, 40 * np.pi, n)
      noise = np.random.normal(size=n)
      tone = np.sin(x)
      fig, axes = plt.subplots(1, 2, constrained_layout=True, sharex=True, sharey=True)
      ax0, ax1 = axes
      ax0.plot(noise, "C0")
      ax0.set_title(r"Noise ($X=0$)", fontsize=14)
      ax1.plot(noise + tone, "C0")
      ax1.set_title(r"Noise plus signal ($X=1$)", fontsize=14)
      for ax in axes:
          ax0.set_xticks([], [])
          ax0.set_yticks([], [])
          sb.despine(fig, ax, top=True, right=True)
          ax.set_xlabel("Time")
          ax.set_ylabel("Pressure")
      ax1.set_xlim(0, n)
      ax1.set_ylim((noise + tone).min(), (noise + tone).max())
      plt.savefig(
          f"../../assets/images/noise_toneplusnoise.svg", bbox_inches=0, transparent=True
      )
qt-sound-example__py: |
  from math import pi, sin
  import struct, sys
  
  from PyQt5.QtCore import QBuffer, QByteArray, QIODevice, Qt
  from PyQt5.QtWidgets import (
      QApplication,
      QFormLayout,
      QLineEdit,
      QHBoxLayout,
      QPushButton,
      QSlider,
      QVBoxLayout,
      QWidget,
  )
  from PyQt5.QtMultimedia import QAudio, QAudioDeviceInfo, QAudioFormat, QAudioOutput
  
  
  class Window(QWidget):
      def __init__(self, parent=None):
  
          QWidget.__init__(self, parent)
  
          format = QAudioFormat()
          format.setChannelCount(1)
          format.setSampleRate(22050)
          format.setSampleSize(16)
          format.setCodec("audio/pcm")
          format.setByteOrder(QAudioFormat.LittleEndian)
          format.setSampleType(QAudioFormat.SignedInt)
          self.output = QAudioOutput(format, self)
  
          self.frequency = 440
          self.volume = 0
          self.buffer = QBuffer()
          self.data = QByteArray()
  
          self.deviceLineEdit = QLineEdit()
          self.deviceLineEdit.setReadOnly(True)
          self.deviceLineEdit.setText(QAudioDeviceInfo.defaultOutputDevice().deviceName())
  
          self.pitchSlider = QSlider(Qt.Horizontal)
          self.pitchSlider.setMaximum(100)
          self.volumeSlider = QSlider(Qt.Horizontal)
          self.volumeSlider.setMaximum(32767)
          self.volumeSlider.setPageStep(1024)
  
          self.playButton = QPushButton(self.tr("&Play"))
  
          self.pitchSlider.valueChanged.connect(self.changeFrequency)
          self.volumeSlider.valueChanged.connect(self.changeVolume)
          self.playButton.clicked.connect(self.play)
  
          formLayout = QFormLayout()
          formLayout.addRow(self.tr("Device:"), self.deviceLineEdit)
          formLayout.addRow(self.tr("P&itch:"), self.pitchSlider)
          formLayout.addRow(self.tr("&Volume:"), self.volumeSlider)
  
          buttonLayout = QVBoxLayout()
          buttonLayout.addWidget(self.playButton)
          buttonLayout.addStretch()
  
          horizontalLayout = QHBoxLayout(self)
          horizontalLayout.addLayout(formLayout)
          horizontalLayout.addLayout(buttonLayout)
  
          self.play()
          self.createData()
  
      def changeFrequency(self, value):
  
          self.frequency = 440 + (value * 2)
          self.createData()
  
      def play(self):
  
          if self.output.state() == QAudio.ActiveState:
              self.output.stop()
  
          if self.buffer.isOpen():
              self.buffer.close()
  
          if self.output.error() == QAudio.UnderrunError:
              self.output.reset()
  
          self.buffer.setData(self.data)
          self.buffer.open(QIODevice.ReadOnly)
          self.buffer.seek(0)
  
          self.output.start(self.buffer)
  
      def changeVolume(self, value):
  
          self.volume = value
          self.createData()
  
      def createData(self):
  
          self.data.clear()
          for i in range(2 * 22050):
              t = i / 22050.0
              value = int(self.volume * sin(2 * pi * self.frequency * t))
              self.data.append(struct.pack("<h", value))
  
  
  if __name__ == "__main__":
  
      app = QApplication(sys.argv)
      window = Window()
      window.show()
      sys.exit(app.exec_())
