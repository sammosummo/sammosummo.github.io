---
layout: post
title: Bayesian hierarchical logistic regression
date: 2017-10-31
categories:
 - Bayesian
description:
image: https://sammosummo.github.io/images/netsuke.jpg
image-sm: https://sammosummo.github.io/images/netsuke-sm.jpg
image-description: "Netsuke at the Metropolitan Museum of Art"

---
Here’s a code snippet I’ve used a couple of times recently. It implements a Bayesian hierarchical logistic regression in [PyMC3](http://docs.pymc.io).

The code conveniently loads in data, builds the model (specified using a [patsy](https://patsy.readthedocs.io/en/latest/)-style design matrix), samples it, and finally saves the samples, posterior parameter estimates, and a couple of figures. It contains some additional convenience features, including automatically naming the coefficients and optionally ‘compressing’ the data set to speed up sampling. 

The code has two outstanding issues that really bug me. The first is that data compression will go awry if the name of any column in the data frame is a substring of the design formula. I haven’t decided on the best way to solve this because, unfortunately, there doesn’t seem to be a clean way to get out the factors in a patsy formula if you are using categorical variables (see [here](https://stackoverflow.com/questions/43378033/how-do-i-get-the-columns-that-a-statsmodels-patsy-formula-depends-on/43378766?noredirect=1#comment80920947_43378766)). The second is the choice of hyperpriors, particularly for the sigmas. Overall, it seems to get the job done.

Here is the gist:

<script src="https://gist.github.com/sammosummo/a169c871c5950255b7d6189973b38ac1.js"></script>
