---
layout: post
title: Easy Bayesian generalization of mulitvariate linear models
date: 2019-10-10
has_comments: true
has_code: true
has_math: true
include_references: true
references:
 - Coull2000a
---
Consider the multivariate version of the general linear model (GLM),

$$\begin{equation}
\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{B}+\boldsymbol{E}\\
\boldsymbol{E}\sim\textrm{MvNormal}\left(0, \boldsymbol{\Sigma}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{Y}$$ is an $$n\times{}m$$ matrix of data with rows indicating independent cases and columns
indicating different variables; $$\boldsymbol{X}$$ is an $$n\times{}k$$ matrix of covariates; $$\boldsymbol{B}$$ is an
$$m\times{}k$$ matrix of coefficients; $$\boldsymbol{E}$$ is an $$n\times{}m$$ matrix of residuals; and
$\boldsymbol{\Sigma}$ is a $$p\times{}p$$ covariance matrix.

For Bayesians, the following equivalent expression may feel more natural:

$$\begin{equation}
\boldsymbol{Y}\sim\textrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B},\boldsymbol{\Sigma}\right)\textrm{.}
\end{equation}$$

In the special case where $$\boldsymbol{\Sigma}$$ is a diagonal matrix, the above model is equivalent to multiple
univariate GLMs, one on each dependent variable. The univariate GLM can be written as

$$\begin{equation}
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\\
\boldsymbol{e}\sim\textrm{Normal}\left(0, \sigma^2\right)\textrm{,}
\end{equation}$$

or equivalently,

$$\begin{equation}
\boldsymbol{y}\sim\textrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, \sigma^2\right)\textrm{.}
\end{equation}$$

Univariate GLMs are simpler and more commonly used than multivariate GLMs. Each univariate GLM can be used in parallel
to the others, which may save time. Thus, if a researcher doesn't care about potential correlations between dependent
variables, or how such correlations might influence estimates of regression coefficients, they may as well use several
univariate GLMs.

Suppose that the data are categorical, ordinal, counts, or bounded between certain values. In such cases, the residuals
may not be well approximated by a normal distribution, making GLMs inappropriate.

When dealing with a single dependent variable, we can use a general*ized* linear model (GZLM). For example, for binary
data, we can use the following model:

$$\begin{equation}
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{.}
\end{equation}$$

In words, this model assumes that (1) the data are Bernoulli trials, and therefore that their likelihood is given by the
probability mass function of the Bernoulli distribution conditional on a set of success probabilities; and (2) the
vector of logits of these probabilities are equal to a linear combination of covariates and coefficients. One could
describe this model as the univariate logistic Bernoulli GZLM, but its common name is logistic regression.

This is all fine, but what do we do if we have a multivariate data set, the correlations between dependent variables are
important, _and_ the data aren't well approximated by a normal distribution?

A common solution is to recast the data matrix $$\boldsymbol{Y}$$ as a single column and use a generalized linear mixed
model (GZLMM) where cases are random effects (e.g., [Breslow & Clayton, 1993](#Breslow1993a)). For example,

$$\begin{equation}
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{u}\right)\\
\boldsymbol{\psi}=\mathrm{vec}\left(\boldsymbol{Y}\right)\\
\boldsymbol{\psi}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{,}
\end{equation}$$

were $$\boldsymbol{Z}$$ is a design matrix and $$\boldsymbol{u}$$ is a vector of coefficients for the $$n$$ random
effects. Here, the random effects act as separate intercepts for each case, which induces positive correlations across
items. One problem with this approach is that this kind of model cannot account for negative correlations across items.

Another solution, which I consider in detail here, is to define a model whose likelihood function is the probability
density/mass function of some conventional distribution conditional on a set parameters, and assume that some
transformation of these parameters follows a multivariate normal distribution whose expectation is a linear
combination of covariates and coefficients.

Importantly, this is not quite the same thing as a multivariate extension of a univariate GZLM, nor is it a GZLMM. To
illustrate, first consider the following univariate model:

$$\begin{equation}
\boldsymbol{\psi}\sim\mathrm{Normal}\left(\boldsymbol{X}\boldsymbol{\beta}, 1\right)\\
\boldsymbol{\pi}=\mathrm{logistic}\left(\boldsymbol{\psi}\right)\\
\boldsymbol{y}\sim\mathrm{Bernoulli}\left(\boldsymbol{\pi}\right)\textrm{.}
\end{equation}$$

The difference between this model and logistic regression is the addition of a latent stochastic variable,
$$\boldsymbol{\psi}$$, whose expectation $$\boldsymbol{X}\boldsymbol{\beta}$$. The variance of this latent variable must
be fixed for the model to be identified, and unit variance is the obvious choice. Hereafter, I refer to models of this
kind as univariate latent-variable models (LVMs).

Within the frequentist framework, fitting univariate LVMs is not as easy as fitting univariate GZLMs because the former
are typically not included within common statistics packages. Fitting LVMs by hand is also more difficult because they
have more complicated likelihood functions. For example, whereas the likelihood function for logistic regression is
simply the probability mass function of the Bernoulli distribution after substituting $$\boldsymbol{\pi}$$ for
$$\mathrm{logistic}\left(\boldsymbol{X}\boldsymbol{\beta}\right)$$, the likelihood function of corresponding LVM
involves an additional step that marginalizes out $$\boldsymbol{\psi}$$. (I've omitted this equation from this post because, frankly, I don't
have the time or patience to derive and typeset it.)

Thankfully, within the Bayesian framework, we can side-step most of the additional difficulty associated with LVMs.
Bayesian software packages, such as PyMC3 and Stan, require the user to define models in terms of the probabilistic 
relationships between variables, so there is no need to write the non-conventional likelihood functions explicitly. The
latent variables within LVMs are simply defined and are estimated using Markov Chain Monte Carlo
(MCMC), along with the other variables in the model.


when operating within the Bayesian framework.

fitting ULVMs is almost as easy as fitting UGLZMs using general software such
as PyMC3 or Stan. Because models in such software there is no need
to derive the marginalized likelihood functionâ€”we can estimate $$\boldsymbol{\psi}$$ with Markov Chain Monte Carlo
(MCMC), along with all the other variables in the model.

Here is a demonstration. The following script does three things: (1) fits a frequentist logistic regression to some data
using statsmodels; (2) fits a Bayesian version of the same model to the same data using PyMC3; and (3) fits a ULVM to
the same data using PyMC3. Notice that the code for (2) and (3) is not very different. The portion of the output related
to (1) is


The output related to (2) is

Notice how the marginal posterior means of $$\boldsymbol{\beta}$$ are very close to their maximum-likelihood estimates
(MLEs) from the frequentist model. Unsurprising, but reassuring.

The output related to (3) is

The marginal posteior means from this model are extremely similar to those from the Bayesian logistic regression.
Moreover, model-fit statistics are almost the same as well. This tells us something important: for all intents and
purposes, the UGZLM and ULVM are equivalent models.

Now let's discuss multivariate generalizations of the ULVMs (MLVMs). The multivariate analog of the ULVM presented
earlier is

$$\begin{equation}
\boldsymbol{\Psi}\sim\mathrm{MvNormal}\left(\boldsymbol{X}\boldsymbol{B}, \boldsymbol{\Theta}\right)\\
\boldsymbol{P}=\mathrm{logistic}\left(\boldsymbol{\Psi}\right)\\
\boldsymbol{Y}\sim\mathrm{Bernoulli}\left(\boldsymbol{P}\right)\textrm{,}
\end{equation}$$

where $$\boldsymbol{\Theta}$$ is an $$m\times{}m$$ correlation matrix (i.e., a covariance matrix with 1s down the
diagonal). Fixing the diagonals is required for the same reason it was necessary to fix the variance of
$$\boldsymbol{\psi}$$ in the ULVM.

MLVMs have extremely complex likelihood functions because they involve intergrals that are multivariate in nature (e.g.,
see Eq. 3 in [Coull and Agresti, 2000](#Coull2000a)). Again, however, MCMC sampling of posteriors in Bayesian MLVMs via
PyMC3 or a similar software (e.g., Stan) does not require us to derive or code up this function explicitly.

Here is some example PyMC3 code to fit the MLVM defined above.

To my mind at least, MLVMs are a natural way to model the relationships between variables when the multivariate normal
assumption is clearly violated. There are many statistical problems that could be framed in terms of MVLMs, such as
as factor analysis, structural equation models, and multi-trait quantitative genetics models. There are, of course, 
numerous competing solutions for achieving the same goal in each of these examples, and many of them are computationally
simpler. In a frequentist framework, deriving and coding up the MLVM likelihood function may be prohibitively taxing. In
a Bayesian framework, however, one simply needs to define the model and sample. This flexibility could represent a major
advantage of MLVMs over other approaches, becuase it is straightforward to create a new model with a rare combination of
link function and likelihood that may not have been implemented in the specialized statistics program you typically use. However, a disadvantage of Bayesian MLVMs is
that they are slow. Sampling from the latent multivariate normal takes time, and due to the extra complexity, many MCMC
samples may needed to obtain a good approximation of the joint posterior distribution.

