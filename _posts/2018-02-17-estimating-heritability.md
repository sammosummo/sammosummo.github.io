---
layout: post
title: Estimating heritability
date: 2018-02-15
categories:
- Genetics
description:
image: https://sammosummo.github.io/images/Pieter_Bruegel_the_Elder-_The_Seven_Deadly_Sins_or_the_Seven_Vices_-_Gluttony.JPG
image-sm: https://sammosummo.github.io/images/Pieter_Bruegel_the_Elder-_The_Seven_Deadly_Sins_or_the_Seven_Vices_-_Gluttony_sm.JPG
image-description: "Guttony (1558) by Pieter Bruegel the Elder"
---

[Previously](https://sammosummo.github.io/2018/02/10/heritability/), I presented the theory behind the method of variance components for estimating heritability. Here, I simulate some data according to the theory, and attempt to recover its parameters using various methods.
 
Suppose we have a trait generated by the process

$$
\mathbf{y} \sim \mathrm{MvNormal}\left(\mathbf{X}\beta, \mathbf{A}\sigma^2_\mathrm{A} + \mathbf{I}\sigma^2_\mathrm{E}\right)
$$

where $$\mathbf{y}$$ is the trait vector, $$\mathbf{X}$$ is an arbitrary design matrix, $$\beta$$ is a vector of coefficients, $$\mathbf{A}$$ is an arbitrary kinship matrix, $$\sigma^2_\mathrm{A}$$ is the additive genetic variance, $$\mathbf{I}$$ is an identity matrix, and $$\sigma^2_\mathrm{E}$$ is the residual variance. As a reminder, the narrow-sense heritability is the proportion of the total variance explained by additive genetic factors, or

$$
h^2 = \frac{\sigma^2_\textrm{A}}{\left(\sigma^2_\textrm{A}+\sigma^2_\textrm{E}\right)}
$$

The following Python code will generate a trait with a heritability of 0.75 for 500 individuals:
 
~~~ python
import numpy as np

β = np.array([[1, 2, 3]])
σA = 9
σE = 3
# h2 = 9 / (9 + 3) = 0.75
n = 500
X = np.random.rand(n, len(β))
X[:, 0] = np.ones(n)  # design mtrx w/ intcpt
r = np.random.randn
A = np.array([r(n) + r(1) for i in range(n)])
A = np.dot(A, np.transpose(A))
Dh = np.diag(np.diag(A) ** -0.5)
A = np.dot(Dh, np.dot(A, Dh))  # kinship mtrx
mvn = np.random.multivariate_normal
u = mvn(np.zeros(n), A * σA ** 2)
e = np.random.normal(0, σE, n)
y = np.dot(X, β) + u + e  # trait
~~~

To recover the heritability, we first need the maximum-likelihood estimates of $$\beta$$, $$\sigma_\mathrm{A}$$, and $$\sigma_\mathrm{e}$$. A pragmatic solution to this problem is to use SciPy’s optimisation routines to iteratively minimise the negative log likelihood:

~~~python
from scipy.stats import multivariate_normal
from scipy.optimize import minimize

def f(θ):
    *_β, _σA, _σE = θ
    μ = np.dot(X, _β)
    I = np.eye(len(y)
    Σ = np.dot(A, _σA ** 2) + np.dot(I, _σE ** 2)
    return -multivariate_normal.logpdf(y, μ, Σ)

x0 = np.ones(X.shape[1] + 2)
result = minimize(f, x0, method='L-BFGS-B')
~~~

This produced a heritability estimate of 0.76 in about 6 seconds on my MacBook Air (2011). Not bad for illustrative purposes, but this solution is impractically slow for real-world data sets with thousands — if not tens or even hundreds of thousands — of data points. It also doesn’t help improve our understanding because the most important steps (calculation of the log likelihood and the minimisation routine) are done under-the-hood by SciPy.
