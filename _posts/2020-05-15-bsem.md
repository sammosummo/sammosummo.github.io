---
date: 2020-05-15
has_code: true
has_comments: true
has_math: true
has_references: true
include_references: true
layout: post
references:
- Bollen1989a
- Holzinger1939a
- Joreskog1969a
- Muthen2012a
- Salvatier2016a
- Spearman1904a
short_title: Bayesian structural equation modeling II
tags:
- python
- bayesian
title: 'Bayesian structural equation modeling II: Hierarchical latent variables'
---

[Previously](bcfa), I presented code to perform Bayesian confirmatory factor analysis (CFA; {{ site.data.refs.Joreskog1969a.citenp }})
using [PyMC3](https://docs.pymc.io/) {{ site.data.refs.Salvatier2016a.citep }}. In the preamble, I mentioned that CFA is
a special case of structural equation modeling (SEM; {{ site.data.refs.Bollen1989a.citenp }}). In practice, we use the term
CFA when the models try to explain the covariance between a collection of observed variables (items) using a smaller number
of latent variables (factors). The term SEM is used when the models contain a more elaborate structure of latent variables.

A classic example of SEM is when a CFA model of cognitive data is modified so that it contains an additional latent variable
that influences the other factors but not the items directly. The additional latent variable is interpreted as general
cognitive ability and denoted by $$g$$ {{ site.data.refs.Spearman1904a.citep }}. Such a model may be called *hierarchical* because
it contains two levels of latent variables. Here, I implement a hierarchical model here using
the {{ site.data.refs.Holzinger1939a.citet }} data set.


![](/assets/images/hs-1.gv.svg)
*Relationships between variables in the hierarchical model.*

## Building the model

This particular model requires three inputs. The first is a matrix of data denoted by $$\boldsymbol{Y}$$ with shape $$n
\times p$$, where $$n$$ is the number of cases and $$p$$ is the number of items, just as in the [previous CFA](bcfa).

The second is a latent variable/factor design matrix denoted by $$\boldsymbol{M}$$ with shape $$p \times m$$. This is similar
but not identical to $$\boldsymbol{M}$$ from the previous CFA. Specifically, it contains an extra column for out latent
variable $$g$$. Since this latent variable isn't directly connected to any items, its column contains only 0s.

The third, entirely new input is a matrix of paths between latent variables denoted by $$\boldsymbol{B}$$
with shape $$m \times m$$. If the value of the cell in row $$i$$ and column $$j$$ is 1, the $$j$$th latent variable
influences the $$i$$th latent variable. This matrix can be asymmetric and must contain zeros down the diagonal.

As in the previous CFA model, we assume the data are multivariate normal,

$$\begin{equation}
\boldsymbol{Y}\sim\mathrm{Normal}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)
\end{equation}$$

where $$\boldsymbol{\mu}$$ is a $$p$$-length vector of means and $$\boldsymbol{\Sigma}$$ is an $$n \times p$$
covariance matrix. For former is given by

$$\begin{align}
\boldsymbol{\mu}=\boldsymbol{\nu}+\boldsymbol{\Lambda}\boldsymbol{\alpha}
\end{align}$$

where $$\boldsymbol{\nu}$$ is $$p$$-length vector of item intercepts, $$\boldsymbol{\Lambda}$$ is a $$p \times m$$
matrix of factor loadings, and $$\boldsymbol{\alpha}$$ is a $$m$$-length vector of factor intercepts.

The covariance matrix of our new model is more complex than the one from the previous CFA model:

$$\begin{align}
\boldsymbol{\Sigma}=\boldsymbol{\Lambda}(\boldsymbol{I}-\boldsymbol{\Gamma})^{-1}\boldsymbol{\Psi}(\boldsymbol{I}-\boldsymbol{\Gamma}^\mathrm{T})^{-1}\boldsymbol{\Lambda}^\mathrm{T}+\boldsymbol{\Theta}
\end{align}$$

where $$\boldsymbol{\Psi}$$ is an $$m \times m$$ factor covariance matrix, $$\boldsymbol{I}$$ is an identity matrix of
the same shape, $$\boldsymbol{\Gamma}$$ is a matrix of path weights, and $$\boldsymbol{\Theta}$$ is a
$$p \times p$$ item covariance matrix.

Under the new model, $$\boldsymbol{\Psi}$$ is different than it was previously. Notice that SEM is flexible
enough to estimate both covariances between all factors/latent variables via $$\boldsymbol{\Psi}$$ *and* paths between
the same factors/latent variables via $$\boldsymbol{\Gamma}$$. We are interested in explaining the covariance between
factors in terms of the new latent variable, so it doesn't make sense to estimate any factor covariances. Moreover, the
variances of all factors/latent variables should be 1. (Recall that we need to fix something in the model.) So, for the
present purposes, $$\boldsymbol{\Psi}$$ will be an identity matrix.

As under the previous CFA model, our factor design matrix $$\boldsymbol{M}$$ dictates which items load on which factors, and
the matrix $$\boldsymbol{\Lambda}$$ contains the actual loadings. Previously we used the trick described by
{{ site.data.refs.Muthen2012a.citet }} to estimate both loadings and minor (or cross-) loadings via

$$\begin{equation}
\boldsymbol{\Lambda} = \boldsymbol{\Phi}\circ\left[\beta\left(1 - \boldsymbol{M}\right) + \boldsymbol{M}\right]
\end{equation}$$

where $$\boldsymbol{\Phi}$$ is a matrix of unscaled factor loadings, $$\beta$$ is the cross-loading standard deviation,
and $$\circ$$ denotes the Hadamard product. We assigned univariate standard normal priors to all elements in
$$\boldsymbol{\Phi}$$ and a beta prior to $$\beta$$. Under this model, this trick would allow the additional latent
variable to influence the items directlyâ€”we don't want this to happen.  We can prevent all cross-loadings without changing the
model drastically by fixing $$\beta=0$$.

The matrix of path weights $$\boldsymbol{\Gamma}$$ is the same as the input matrix $$\boldsymbol{B}$$ except values of 1
are replaced with values from new random vector, $$\boldsymbol{b}$$. This vector will have a standard normal prior distribution.
I'm not sure how to represent this operation using traditional notation. It should be clear that the length of
$$\boldsymbol{b}$$ is the elementwise sum of $$\boldsymbol{B}$$.

Now all that's left to do is to place prior distributions on $$\boldsymbol{\nu}$$, $$\boldsymbol{\alpha}$$,
and $$\boldsymbol{\Theta}$$. I'll use the same priors for $$\boldsymbol{\nu}$$ and $$\boldsymbol{\alpha}$$, but simplify
the model a bit by making $$\boldsymbol{\Theta}$$ a diagonal matrix.

## Code and results

Below is code to implement the model.

```python
{{ site.data.code.bsem__py }}
```

Here are factor loadings for the Grant-White school:

|          |   Spatial |   Verbal |   Speed |   Memory |   g |
|:---------|----------:|---------:|--------:|---------:|----:|
| Visual   |     0.43  |    0     |   0     |    0     |   0 |
| Cubes    |     0.301 |    0     |   0     |    0     |   0 |
| Paper    |     0.331 |    0     |   0     |    0     |   0 |
| Flags    |     0.42  |    0     |   0     |    0     |   0 |
| General  |     0     |    0.606 |   0     |    0     |   0 |
| Paragrap |     0     |    0.617 |   0     |    0     |   0 |
| Sentence |     0     |    0.632 |   0     |    0     |   0 |
| Wordc    |     0     |    0.522 |   0     |    0     |   0 |
| Wordm    |     0     |    0.636 |   0     |    0     |   0 |
| Addition |     0     |    0     |   0.455 |    0     |   0 |
| Code     |     0     |    0     |   0.482 |    0     |   0 |
| Counting |     0     |    0     |   0.49  |    0     |   0 |
| Straight |     0     |    0     |   0.526 |    0     |   0 |
| Wordr    |     0     |    0     |   0     |    0.354 |   0 |
| Numberr  |     0     |    0     |   0     |    0.356 |   0 |
| Figurer  |     0     |    0     |   0     |    0.403 |   0 |
| Object   |     0     |    0     |   0     |    0.431 |   0 |
| Numberf  |     0     |    0     |   0     |    0.431 |   0 |
| Figurew  |     0     |    0     |   0     |    0.33  |   0 |


And here are the paths:

|         |   Spatial |   Verbal |   Speed |   Memory |     g |
|:--------|----------:|---------:|--------:|---------:|------:|
| Spatial |         0 |        0 |       0 |        0 | 1.293 |
| Verbal  |         0 |        0 |       0 |        0 | 0.888 |
| Speed   |         0 |        0 |       0 |        0 | 1.032 |
| Memory  |         0 |        0 |       0 |        0 | 1.1   |
| g       |         0 |        0 |       0 |        0 | 0     |